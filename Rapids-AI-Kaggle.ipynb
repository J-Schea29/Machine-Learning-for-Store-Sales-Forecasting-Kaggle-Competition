{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fdbfd5-d749-441e-9291-48c9e26dfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e042b-275d-4a9b-85d8-77d00104f47c",
   "metadata": {},
   "source": [
    "## Imports Needed to run Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95e2ccd0-6df8-4ca5-96b1-58a118dd875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for data wrangling \n",
    "import cudf\n",
    "import cupy as cp\n",
    "import pandas\n",
    "import numpy as np\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35ba9ed7-f231-4876-b860-c28fae1a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing given data\n",
    "train = cudf.read_csv(\"./input/train.csv\", parse_dates=['date'])\n",
    "\n",
    "test = cudf.read_csv(\"./input/test.csv\", parse_dates=['date'])\n",
    "\n",
    "oil = cudf.read_csv(\"./input/oil.csv\", parse_dates=['date'])\n",
    "\n",
    "holiday = cudf.read_csv(\"./input/holidays_events.csv\")\n",
    "\n",
    "store = cudf.read_csv(\"./input/stores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db416fb9-d8e8-4b59-a7a9-9aae161d54fe",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f704e17-0daa-43ac-82b1-83c6b0c916e9",
   "metadata": {},
   "source": [
    "### Capturing Seasonal Holiday Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a05e50e-ddda-4e59-9fa9-b2f39bf7d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dates to datetime\n",
    "holiday[\"date\"] = cudf.to_datetime(holiday[\"date\"], format='%Y-%m-%d')\n",
    "holiday = holiday.set_index(\"date\")\n",
    "\n",
    "# Keeping only celbrated holidays\n",
    "holiday = holiday.loc[(holiday[\"transferred\"]!=True)].drop(\"transferred\", axis=1)\n",
    "holiday.loc[holiday[\"type\"]==\"Transfer\", \"type\"] = \"Holiday\"\n",
    "\n",
    "# Bridged days are day where there is no work\n",
    "bridge = holiday.loc[holiday[\"type\"]==\"Bridge\"]\n",
    "bridge[\"bridge\"] = True\n",
    "bridge = bridge[[\"bridge\"]]\n",
    "\n",
    "# Special events\n",
    "event = holiday.loc[holiday[\"type\"]==\"Event\"][[\"description\"]]\n",
    "\n",
    "# Keeping only holidays\n",
    "holiday = holiday.loc[holiday[\"type\"]==\"Holiday\"]\n",
    "\n",
    "# Holidays celerbated localy \n",
    "loc_hol = holiday.loc[holiday[\"locale\"]==\"Local\"][[\"locale_name\", \"description\"]]\n",
    "\n",
    "# Holidays celerbrated regionally\n",
    "reg_hol = holiday.loc[holiday[\"locale\"]==\"Regional\"][[\"locale_name\", \"description\"]]\n",
    "\n",
    "#Holidays celberbrated nationally\n",
    "nat_hol = holiday.loc[holiday[\"locale\"]==\"National\"][[\"description\"]]\n",
    "\n",
    "# Recording days Earthquake\n",
    "quake = event.loc[event[\"description\"].str.find(\"Terremoto Manabi\")!=-1]\n",
    "quake[\"time_since_quake\"] = cp.arange(1,len(quake.index)+1)\n",
    "quake.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "# Removing Earthquake and adding Sporting Events\n",
    "event = event.loc[event[\"description\"].str.find(\"Terremoto Manabi\")==-1]\n",
    "event.loc[event[\"description\"].str.find(\"futbol\")!=-1, \"description\"]= \"Sports\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f7c2d-63a5-4f01-bfcb-553480b34fd3",
   "metadata": {},
   "source": [
    "### Location Specific Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e484d5b1-1842-465e-ae7c-51474ccc5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper format\n",
    "train[\"store_nbr\"] = train[\"store_nbr\"].astype(int)\n",
    "\n",
    "# Merging\n",
    "X = train.merge(store, on=\"store_nbr\", how=\"left\")\n",
    "X.drop(\"cluster\", axis=1, inplace=True)\n",
    "\n",
    "# Converting dates to datetime\n",
    "X[\"date\"] = cudf.to_datetime(X[\"date\"], format='%Y-%m-%d')\n",
    "\n",
    "# Creating feature measuring the total in store promotions.\n",
    "total_other_promo_store = X[[\"date\", \"store_nbr\", \"onpromotion\"]].groupby(['date', 'store_nbr']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_promo_store = total_other_promo_store.rename(columns={'onpromotion': 'total_other_promo_store',})\n",
    "\n",
    "# Creating feature measuring the total promotions in each town for similar products.\n",
    "total_other_city_promo = X[[\"date\", \"onpromotion\", \"family\", \"city\"]].groupby(['date', 'city', 'family']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_city_promo = total_other_city_promo.rename(columns={'onpromotion': 'total_other_city_promo',})\n",
    "\n",
    "# Adding new features\n",
    "X = X.merge(total_other_promo_store, on=['date', 'store_nbr'], how=\"left\")\n",
    "X = X.merge(total_other_city_promo, on=['date', 'city', 'family'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4df2e7df-a73d-4a5d-88bb-04e2b33052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper format\n",
    "store[\"store_nbr\"] = store[\"store_nbr\"].astype(int)\n",
    "test[\"store_nbr\"] = test[\"store_nbr\"].astype(int)\n",
    "\n",
    "# Merging\n",
    "X_test = test.merge(store, on=\"store_nbr\", how=\"left\")\n",
    "X_test.drop(\"cluster\", axis=1, inplace=True)\n",
    "\n",
    "# Converting dates to datetime\n",
    "X_test[\"date\"] = cudf.to_datetime(X_test[\"date\"], format='%Y-%m-%d')\n",
    "\n",
    "# Creating feature measuring the total in store promotions.\n",
    "total_other_promo_store = X_test[[\"date\", \"store_nbr\", \"onpromotion\"]].groupby(['date', 'store_nbr']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_promo_store = total_other_promo_store.rename(columns={'onpromotion': 'total_other_promo_store',})\n",
    "\n",
    "# Creating feature measuring the total promotions in each town for similar products.\n",
    "total_other_city_promo = X_test[[\"date\", \"onpromotion\", \"family\", \"city\"]].groupby(['date', 'city', 'family']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_city_promo = total_other_city_promo.rename(columns={'onpromotion': 'total_other_city_promo',})\n",
    "\n",
    "# Adding new features\n",
    "X_test = X_test.merge(total_other_promo_store, on=['date', 'store_nbr'], how=\"left\")\n",
    "X_test = X_test.merge(total_other_city_promo, on=['date', 'city', 'family'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d76ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.set_index(\"date\")\n",
    "X_test = X_test.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a203c43-19df-48ba-9c9c-f6b423a2dd40",
   "metadata": {},
   "source": [
    "### Merging with Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e576cca1-b3eb-455f-aed1-65a679476c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding national holidays\n",
    "X = X.merge(nat_hol, on=\"date\", how=\"left\")\n",
    "\n",
    "# Bridge days\n",
    "X = X.merge(bridge, on=\"date\", how=\"left\")\n",
    "\n",
    "# Adding local holdays\n",
    "X = X.merge(loc_hol, left_on=[\"date\", \"city\"],\n",
    "            right_on=[\"date\", \"locale_name\"],\n",
    "            suffixes=(None, '_l'), how=\"left\"\n",
    "           )\n",
    "X.drop(\"locale_name\", axis=1, inplace=True)\n",
    "\n",
    "# Adding regional holidays\n",
    "X = X.merge(reg_hol, left_on=[\"date\", \"state\"],\n",
    "            right_on=[\"date\", \"locale_name\"], \n",
    "            suffixes=(None, '_r'),how=\"left\"\n",
    "           )\n",
    "X.drop(\"locale_name\", axis=1, inplace=True)\n",
    "\n",
    "# True if holiday that Day\n",
    "X[\"holiday\"] = (((X[\"descriptionNone\"].isnull()==False) | (X[\"description_l\"].isnull()==False)) | (X[\"description\"].isnull()==False))\n",
    "\n",
    "# Combine Holiday descriptions\n",
    "X.drop(\"descriptionNone\", axis=1, inplace=True)\n",
    "X.drop(\"description_l\", axis=1, inplace=True)\n",
    "X.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "#Events\n",
    "X = X.merge(event, on=\"date\", how=\"left\")\n",
    "X = X.rename(columns={'description': 'event',})\n",
    "X[\"event\"] = X[\"event\"].fillna(\"none\")\n",
    "\n",
    "# Adding Quake data\n",
    "X = X.merge(quake, on=\"date\", how=\"left\")\n",
    "X[\"time_since_quake\"] = X[\"time_since_quake\"].fillna(0)\n",
    "\n",
    "#To model a diminishing marginal effect on the economy by the earthquake\n",
    "X[\"time_since_quake_sq\"] = X[\"time_since_quake\"]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e60cf2ae-a8ab-49ee-9b2c-354f2c34131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding national holidays\n",
    "X_test = X_test.merge(nat_hol, on=\"date\", how=\"left\")\n",
    "del nat_hol\n",
    "\n",
    "# Bridge days\n",
    "X_test = X_test.merge(bridge, on=\"date\", how=\"left\")\n",
    "del bridge\n",
    "\n",
    "# Adding local holdays\n",
    "X_test = X_test.merge(loc_hol, left_on=[\"date\", \"city\"],\n",
    "            right_on=[\"date\", \"locale_name\"],\n",
    "            suffixes=(None, '_l'), how=\"left\"\n",
    "           )\n",
    "X_test.drop(\"locale_name\", axis=1, inplace=True)\n",
    "del loc_hol\n",
    "\n",
    "# Adding regional holidays\n",
    "X_test = X_test.merge(reg_hol, left_on=[\"date\", \"state\"],\n",
    "            right_on=[\"date\", \"locale_name\"], \n",
    "            suffixes=(None, '_r'),how=\"left\"\n",
    "           )\n",
    "X_test.drop(\"locale_name\", axis=1, inplace=True)\n",
    "del reg_hol\n",
    "\n",
    "# True if holiday that Day\n",
    "X_test[\"holiday\"] = (((X_test[\"descriptionNone\"].isnull()==False) | (X_test[\"description_l\"].isnull()==False)) | (X_test[\"description\"].isnull()==False))\n",
    "\n",
    "# Combine Holiday descriptions\n",
    "X_test.drop(\"descriptionNone\", axis=1, inplace=True)\n",
    "X_test.drop(\"description_l\", axis=1, inplace=True)\n",
    "X_test.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "#Events\n",
    "X_test = X_test.merge(event, on=\"date\", how=\"left\")\n",
    "X_test = X_test.rename(columns={'description': 'event',})\n",
    "X_test[\"event\"] = X_test[\"event\"].fillna(\"none\")\n",
    "del event\n",
    "\n",
    "# Adding Quake data\n",
    "X_test = X_test.merge(quake, on=\"date\", how=\"left\")\n",
    "X_test[\"time_since_quake\"] = X_test[\"time_since_quake\"].fillna(0)\n",
    "del quake\n",
    "\n",
    "#To model a diminishing marginal effect on the economy by the earthquake\n",
    "X_test[\"time_since_quake_sq\"] = X_test[\"time_since_quake\"]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059c7b0-ce19-4011-929d-cd39e25682df",
   "metadata": {},
   "source": [
    "### Merging with Oil Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9d6f820-485e-4f44-a3ef-821638b194ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil[\"date\"] = cudf.to_datetime(oil[\"date\"], format='%Y-%m-%d')\n",
    "oil = oil.set_index(\"date\")\n",
    "X = X.merge(oil, on=\"date\", how=\"left\")\n",
    "X_test = X_test.merge(oil, on=\"date\", how=\"left\")\n",
    "\n",
    "del oil\n",
    "\n",
    "# There is no price of oil on days that the market is closed.\n",
    "# To fill the price, we first fill with the last value.\n",
    "X[\"dcoilwtico\"]= X[\"dcoilwtico\"].ffill()\n",
    "X_test[\"dcoilwtico\"]= X_test[\"dcoilwtico\"].ffill()\n",
    "\n",
    "# We back fill just for first couple values that are empty.\n",
    "X[\"dcoilwtico\"]= X[\"dcoilwtico\"].bfill()\n",
    "X_test[\"dcoilwtico\"]=X_test[\"dcoilwtico\"].bfill()\n",
    "\n",
    "# I just to do a rolling average to smooth out any problems with the empty values,\n",
    "# and to capture any effect of changes. \n",
    "X[\"dcoilwtico\"] = X[\"dcoilwtico\"].rolling(\n",
    "    window=30,       \n",
    "    min_periods=1,  \n",
    ").mean()\n",
    "\n",
    "X_test[\"dcoilwtico\"] = X_test[\"dcoilwtico\"].rolling(\n",
    "    window=30,       \n",
    "    min_periods=1,  \n",
    ").mean()\n",
    "\n",
    "\n",
    "#oil.oil = oil.oil.interpolate(method=\"linear\", limit_direction=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5cddac-bc83-4fce-903a-ce35d9675dcc",
   "metadata": {},
   "source": [
    "### Time Based Varriables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ef2ed5c-e767-476d-b9cc-50d06717e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variables\n",
    "X[\"day\"] = X.index.dayofweek\n",
    "X[\"dayofyear\"] = X.index.dayofyear\n",
    "X[\"month\"] = X.index.month\n",
    "X[\"year\"] = X.index.year\n",
    "\n",
    "# This varible says whether it is a workday.\n",
    "X[\"workday\"] = (((X.bridge.isnull()) & (X.holiday==False)) & ((X[\"day\"]!=5) & (X[\"day\"]!=6)))\n",
    "X.drop(\"bridge\", axis=1, inplace=True)\n",
    "\n",
    "# In Ecudor, people get paid on the 15 and the last day of the month\n",
    "X[\"payday\"] = ((X.index.day==15) | (X.index.day==X.index.to_series().dt.days_in_month)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a7d9e6e-6b89-4dfd-b375-aa0a56cd1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variables\n",
    "X_test[\"day\"] = X_test.index.dayofweek\n",
    "X_test[\"dayofyear\"] =X_test.index.dayofyear\n",
    "X_test[\"month\"] = X_test.index.month\n",
    "X_test[\"year\"] = X_test.index.year\n",
    "\n",
    "# This varible says whether it is a workday.\n",
    "X_test[\"workday\"] = (((X_test.bridge.isnull()) & (X_test.holiday==False)) & ((X_test[\"day\"]!=5) & (X_test[\"day\"]!=6)))\n",
    "X_test.drop(\"bridge\", axis=1, inplace=True)\n",
    "\n",
    "# In Ecudor, people get paid on the 15 and the last day of the month\n",
    "X_test[\"payday\"] = ((X_test.index.day==15) | (X_test.index.day==X_test.index.to_series().dt.days_in_month)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9d618-c5d0-4c41-8d31-e255fdba0d75",
   "metadata": {},
   "source": [
    "### Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2deef0a-75d4-4134-9c4e-1f0fb03537c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing data type\n",
    "X_test = X_test.reset_index()\n",
    "X_test = X_test.set_index(\"date\")\n",
    "\n",
    "X_test[\"family\"] = X_test[\"family\"]\n",
    "X_test[\"store_nbr\"] = X_test[\"store_nbr\"]\n",
    "X_test[\"holiday\"] = X_test[\"holiday\"]\n",
    "X_test[\"event\"] = X_test[\"event\"]\n",
    "X_test[\"city\"] = X_test[\"city\"]\n",
    "X_test[\"state\"] = X_test[\"state\"]\n",
    "X_test[\"type\"] = X_test[\"type\"]\n",
    "X_test[\"workday\"] = X_test[\"workday\"]\n",
    "X_test[\"payday\"] = X_test[\"payday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "968334b4-6d18-4dd3-b646-d25286f5cccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>total_other_promo_store</th>\n",
       "      <th>total_other_city_promo</th>\n",
       "      <th>holiday</th>\n",
       "      <th>event</th>\n",
       "      <th>time_since_quake</th>\n",
       "      <th>time_since_quake_sq</th>\n",
       "      <th>dcoilwtico</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>workday</th>\n",
       "      <th>payday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>9264</td>\n",
       "      <td>19</td>\n",
       "      <td>MEATS</td>\n",
       "      <td>0</td>\n",
       "      <td>Guaranda</td>\n",
       "      <td>Bolivar</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.81</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>9265</td>\n",
       "      <td>19</td>\n",
       "      <td>PERSONAL CARE</td>\n",
       "      <td>0</td>\n",
       "      <td>Guaranda</td>\n",
       "      <td>Bolivar</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.81</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>9266</td>\n",
       "      <td>19</td>\n",
       "      <td>PET SUPPLIES</td>\n",
       "      <td>0</td>\n",
       "      <td>Guaranda</td>\n",
       "      <td>Bolivar</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.81</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>9267</td>\n",
       "      <td>19</td>\n",
       "      <td>PLAYERS AND ELECTRONICS</td>\n",
       "      <td>0</td>\n",
       "      <td>Guaranda</td>\n",
       "      <td>Bolivar</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.81</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>9268</td>\n",
       "      <td>19</td>\n",
       "      <td>POULTRY</td>\n",
       "      <td>0</td>\n",
       "      <td>Guaranda</td>\n",
       "      <td>Bolivar</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.81</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id store_nbr                   family  onpromotion      city  \\\n",
       "date                                                                         \n",
       "2013-01-06  9264        19                    MEATS            0  Guaranda   \n",
       "2013-01-06  9265        19            PERSONAL CARE            0  Guaranda   \n",
       "2013-01-06  9266        19             PET SUPPLIES            0  Guaranda   \n",
       "2013-01-06  9267        19  PLAYERS AND ELECTRONICS            0  Guaranda   \n",
       "2013-01-06  9268        19                  POULTRY            0  Guaranda   \n",
       "\n",
       "              state type  total_other_promo_store  total_other_city_promo  \\\n",
       "date                                                                        \n",
       "2013-01-06  Bolivar    C                        0                       0   \n",
       "2013-01-06  Bolivar    C                        0                       0   \n",
       "2013-01-06  Bolivar    C                        0                       0   \n",
       "2013-01-06  Bolivar    C                        0                       0   \n",
       "2013-01-06  Bolivar    C                        0                       0   \n",
       "\n",
       "           holiday event  time_since_quake  time_since_quake_sq  dcoilwtico  \\\n",
       "date                                                                          \n",
       "2013-01-06   False  none                 0                    0       93.81   \n",
       "2013-01-06   False  none                 0                    0       93.81   \n",
       "2013-01-06   False  none                 0                    0       93.81   \n",
       "2013-01-06   False  none                 0                    0       93.81   \n",
       "2013-01-06   False  none                 0                    0       93.81   \n",
       "\n",
       "            day  dayofyear  month  year workday payday  \n",
       "date                                                    \n",
       "2013-01-06    6          6      1  2013   False  False  \n",
       "2013-01-06    6          6      1  2013   False  False  \n",
       "2013-01-06    6          6      1  2013   False  False  \n",
       "2013-01-06    6          6      1  2013   False  False  \n",
       "2013-01-06    6          6      1  2013   False  False  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reset_index()\n",
    "\n",
    "X = X.set_index(\"date\")\n",
    "\n",
    "X[\"family\"] = X[\"family\"].astype('category')\n",
    "X[\"store_nbr\"] = X[\"store_nbr\"].astype('category')\n",
    "X[\"holiday\"] = X[\"holiday\"].astype('category')\n",
    "X[\"event\"] = X[\"event\"].astype('category')\n",
    "X[\"city\"] = X[\"city\"].astype('category')\n",
    "X[\"state\"] = X[\"state\"].astype('category')\n",
    "X[\"type\"] = X[\"type\"].astype('category')\n",
    "X[\"workday\"] = X[\"workday\"].astype('category')\n",
    "X[\"payday\"] = X[\"payday\"].astype('category')\n",
    "\n",
    "y = X[[\"store_nbr\", \"family\", \"sales\"]]\n",
    "X.drop(\"sales\", axis=1, inplace=True)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a037ac91-c2d5-4aa7-af0e-875f1a941da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing early time with NaNs\n",
    "X = X.loc[X.index >= \"2015-07-01\"]\n",
    "y = y.loc[y.index >= \"2015-07-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705516e1-83e3-42c1-a5be-f4de5165a39f",
   "metadata": {},
   "source": [
    "## Trainning Model\n",
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b0a9675-4fb0-40b4-b8a7-e47276cb0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Time series functions\n",
    "#from jons_time_series_functions import Prepare_data, Hybrid_Time_Series_ML, Hybrid_Pipeline\n",
    "\n",
    "# Data Preprocessing \n",
    "# from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "\n",
    "# Cross-Validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Models\n",
    "# from sklearn.dummy import DummyRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "# from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50754687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing \n",
    "from cuml.dask.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from cuml.preprocessing import MinMaxScaler, StandardScaler, SimpleImputer, LabelEncoder, OneHotEncoder\n",
    "from cuml.compose import make_column_transformer\n",
    "from statsmodels.tsa.deterministic import CalendarFourier\n",
    "\n",
    "# Cross-Validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from cuml.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from cuml.metrics import mean_squared_error, mean_squared_log_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from jons_time_series_functions import Prepare_data, Hybrid_Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f29f4",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef4cf620-0ef7-4ab0-b3df-f3df15b17564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPUs detected: 1\n",
      "\n",
      "Device 0: NVIDIA GeForce RTX 4070\n",
      "  Total memory: 12.878086144 GB\n",
      "  CUDA Capability: 8.9\n",
      "  MultiProcessor Count: 46\n",
      "  Performing computation on Device 0...\n",
      "\n",
      "Summary of Results:\n",
      "Result from GPU 0: 250273120.00\n",
      "\n",
      "Average result from all GPUs: 250273120.00\n"
     ]
    }
   ],
   "source": [
    "# Function to perform a basic operation on the GPU and return the result\n",
    "def basic_gpu_operation(device):\n",
    "    # Create a random tensor of size (1000, 1000) on the specified device\n",
    "    x = torch.rand((1000, 1000), device=device)\n",
    "    # Perform a basic arithmetic operation (e.g., matrix multiplication with its transpose)\n",
    "    result = torch.matmul(x, x.t())\n",
    "    # Return the sum of the result to ensure a scalar value is returned\n",
    "    return result.sum()\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Print the total number of GPUs detected\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f'Total GPUs detected: {gpu_count}\\n')\n",
    "    # Initialize a list to hold the results from each GPU\n",
    "    results = []\n",
    "    # Loop through all available GPUs, print their properties, perform computations, and gather results\n",
    "    for i in range(gpu_count):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"Device {i}: {gpu_properties.name}\")\n",
    "        print(f\"  Total memory: {gpu_properties.total_memory / 1e9} GB\")\n",
    "        print(f\"  CUDA Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "        print(f\"  MultiProcessor Count: {gpu_properties.multi_processor_count}\")\n",
    "        print(f'  Performing computation on Device {i}...\\n')\n",
    "        # Perform the basic operation on the GPU and append the result to the results list\n",
    "        result = basic_gpu_operation(device)\n",
    "        results.append(result.item())  # Convert to Python number and append\n",
    "    # Summarize and print the results from each GPU\n",
    "    print('Summary of Results:')\n",
    "    for i, result in enumerate(results):\n",
    "        print(f'Result from GPU {i}: {result:.2f}')\n",
    "    # Perform some aggregation on the CPU (e.g., compute the average of all results)\n",
    "    results = cp.array(results)\n",
    "    average_result = cp.mean(results)\n",
    "    print(f'\\nAverage result from all GPUs: {average_result:.2f}')\n",
    "    # Optionally, provide a summary of overall GPU utilization or performance here\n",
    "    # This could involve more detailed metrics based on your specific use case or application\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "465ff6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse=nn.MSELoss().to(device)\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred+1), torch.log(actual + 1))\n",
    "    \n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_layer, n_hidden_1, n_hidden_2, drop):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.input_layer = input_layer\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        \n",
    "        # Layers: Linear, LSTM, Linear\n",
    "        self.linear1 = nn.Linear(input_layer, n_hidden_1)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.lstm = nn.LSTM(n_hidden_1, n_hidden_2, batch_first=True)\n",
    "        self.linear2 = nn.Linear(n_hidden_2, 1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output, (h_t, c_t) = self.lstm(x)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.ReLU(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LSTMRegressor():\n",
    "    def __init__(self, n_hidden=50, n_hidden_2=20, drop=0.2, epochs=100, early_stop=5, lr=0.01, Boosted=False):\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.drop = drop\n",
    "        if Boosted:\n",
    "            self.criterion = nn.MSELoss().to(device)\n",
    "        else: \n",
    "            self.criterion = MSLELoss()\n",
    "            \n",
    "        self.early_stop = early_stop \n",
    "        self.epochs = epochs \n",
    "        self.lr = lr\n",
    "        self.min_val_loss = float('inf')\n",
    "        self.min_val_loss_2 = float('inf')\n",
    "        \n",
    "    def train(self, train_loader):\n",
    "        \n",
    "        self.model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch, y_batch\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(x_batch)\n",
    "            loss = self.criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    def pred(self, test_loader, valid=False, epoch=0):\n",
    "        \n",
    "        self.model.eval()\n",
    "        if valid:\n",
    "             \n",
    "            val_losses = 0\n",
    "            num = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in test_loader:\n",
    "                    x_batch = x_batch\n",
    "                    outputs = self.model(x_batch)\n",
    "\n",
    "                    loss = self.criterion(outputs, y_batch)\n",
    "                    val_losses=+loss.item()\n",
    "\n",
    "                    num=+1\n",
    "\n",
    "            val_loss = val_losses/num\n",
    "\n",
    "            if val_loss<self.min_val_loss:\n",
    "            \n",
    "                self.min_val_loss = val_loss\n",
    "                self.early_stop_count = 0\n",
    "            else:\n",
    "                self.early_stop_count+=1\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Validation score of {np.sqrt(val_loss):.4f}\")\n",
    "            if self.early_stop_count>=self.early_stop:\n",
    "                print(f\"early stopping at Validation Score of {np.sqrt(self.min_val_loss):.4f}\")\n",
    "                print()\n",
    "                self.stop = True\n",
    "\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions = []\n",
    "                for x_batch in test_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    outputs = self.model(x_batch)\n",
    "                    predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "                return np.concatenate(predictions)\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \n",
    "        if isinstance(X, list):\n",
    "            X_train, y_train = X[0], y[0]\n",
    "            self.model = LSTMModel(X_train.shape[1], n_hidden_1=self.n_hidden, n_hidden_2= self.n_hidden_2, drop=self.drop).to(device)\n",
    "            \n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "            train_loader = DataLoader(TensorDataset(X_train.to(device), y_train.to(device)), batch_size=31, shuffle=False)\n",
    "            \n",
    "            X_valid, y_valid = X[1], y[1]\n",
    "            test_loader = DataLoader(TensorDataset(X_valid.to(device), y_valid.to(device)), batch_size=31, shuffle=False)\n",
    "            \n",
    "            self.stop=False\n",
    "            self.early_stop_count =0 \n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                self.train(train_loader)\n",
    "                \n",
    "                self.pred(test_loader, valid=True, epoch=epoch)\n",
    "                if self.stop:\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "                X_train, y_train = X, y\n",
    "                self.model = LSTMModel(X_train.shape[1], n_hidden_1=self.n_hidden, n_hidden_2= self.n_hidden_2, drop=self.drop).to(device)\n",
    "                \n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "                train_loader = DataLoader(TensorDataset(X_train.to(device), y_train.to(device)), batch_size=31, shuffle=False)\n",
    "                \n",
    "                for epoch in range(self.epochs):\n",
    "                    self.train(train_loader)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        test_loader = DataLoader(X.to(device), batch_size=31, shuffle=False)\n",
    "        \n",
    "        outputs = self.pred(test_loader)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd09e7",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "002a6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing steps\n",
    "numeric_transformer = [['dcoilwtico', 'onpromotion', 'total_other_promo_store', 'total_other_city_promo'], StandardScaler()]\n",
    "categorical_transformer = [['event', \"store_nbr\", \"family\"], OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "\n",
    "column_list = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "\n",
    "#data_preprocessor = Prepare_data(column_list, [numeric_transformer, categorical_transformer])\n",
    "data_preprocessor = Prepare_data(column_list, [numeric_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e4a8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = X.copy().drop([\"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1)\n",
    "\n",
    "\n",
    "# X_C[\"family\"] = X_C[\"family\"].cat.codes\n",
    "# X_C[\"store_nbr\"] = X_C[\"store_nbr\"].cat.codes\n",
    "# X[\"holiday\"] = X_C[\"holiday\"].cat.codes\n",
    "# X_C[\"event\"] = X[\"event\"].cat.codes\n",
    "# X_C[\"city\"] = X_C[\"city\"].cat.codes\n",
    "# X_C[\"state\"] = X_C[\"state\"].cat.codes\n",
    "# X_C[\"type\"] = X_C[\"type\"].cat.codes\n",
    "\n",
    "X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_C = X_C.set_index([\"date\"])\n",
    "y = y.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94a2caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_C = X_test.copy().drop([\"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1)\n",
    "X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aab682",
   "metadata": {},
   "source": [
    "## Linear Regression, XGBoost, Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ab7d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(n_estimators, gamma, subsample, max_depth, learning_rate):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    \n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    \n",
    "    params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': max_depth,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': n_estimators,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample}\n",
    "\n",
    "    model_2 = XGBRegressor(**params)\n",
    "    # Use time series split for cross validation. \n",
    "    cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "    \n",
    "    # Create lists to append MSLE scores.\n",
    "    valid_msle = []\n",
    "\n",
    "    # Dates to index through. \n",
    "    dates = X_C.index.drop_duplicates()\n",
    "\n",
    "    \n",
    "    # Perform Cross-Validation to determine how model will do on unseen data.\n",
    "    for train_index, valid_index in cv_split.split(dates):\n",
    "\n",
    "        model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=False)\n",
    "\n",
    "        # Index dates.\n",
    "        date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "        # Selecting data for y_train and y_valid.\n",
    "        y_train = y.loc[date_train]\n",
    "        y_valid = y.loc[date_valid]\n",
    "\n",
    "        # Selecting data for X_train and X_valid.\n",
    "        X_train = X_C.loc[date_train]\n",
    "        X_valid = X_C.loc[date_valid]\n",
    "\n",
    "        X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        X_train = X_train.set_index([\"date\"])\n",
    "        X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "        y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        y_train = y_train.set_index([\"date\"])\n",
    "        y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "        # Fitting model.\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Create predictions for Trainning and Validation.\n",
    "        pred = model.predict(X_valid)\n",
    "\n",
    "        # MSE for trainning and validation. \n",
    "        valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "\n",
    "\n",
    "    return -float(np.sqrt(np.mean(valid_msle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4223669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-1.601   \u001b[0m | \u001b[0m5.085    \u001b[0m | \u001b[0m0.7203   \u001b[0m | \u001b[0m3.001    \u001b[0m | \u001b[0m151.2    \u001b[0m | \u001b[0m0.1468   \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-1.39    \u001b[0m | \u001b[95m3.462    \u001b[0m | \u001b[95m0.1863   \u001b[0m | \u001b[95m6.11     \u001b[0m | \u001b[95m198.4    \u001b[0m | \u001b[95m0.5389   \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-2.172   \u001b[0m | \u001b[0m5.096    \u001b[0m | \u001b[0m0.6852   \u001b[0m | \u001b[0m4.84     \u001b[0m | \u001b[0m439.1    \u001b[0m | \u001b[0m0.02748  \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-1.491   \u001b[0m | \u001b[0m6.352    \u001b[0m | \u001b[0m0.4173   \u001b[0m | \u001b[0m8.028    \u001b[0m | \u001b[0m70.19    \u001b[0m | \u001b[0m0.1982   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-1.681   \u001b[0m | \u001b[0m7.004    \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m5.821    \u001b[0m | \u001b[0m346.2    \u001b[0m | \u001b[0m0.8764   \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m-1.33    \u001b[0m | \u001b[95m7.473    \u001b[0m | \u001b[95m0.08505  \u001b[0m | \u001b[95m3.351    \u001b[0m | \u001b[95m84.92    \u001b[0m | \u001b[95m0.8782   \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-1.457   \u001b[0m | \u001b[0m3.492    \u001b[0m | \u001b[0m0.4211   \u001b[0m | \u001b[0m11.62    \u001b[0m | \u001b[0m266.6    \u001b[0m | \u001b[0m0.6919   \u001b[0m |\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m-1.295   \u001b[0m | \u001b[95m4.578    \u001b[0m | \u001b[95m0.6865   \u001b[0m | \u001b[95m10.51    \u001b[0m | \u001b[95m9.144    \u001b[0m | \u001b[95m0.7502   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-1.71    \u001b[0m | \u001b[0m7.944    \u001b[0m | \u001b[0m0.7482   \u001b[0m | \u001b[0m5.524    \u001b[0m | \u001b[0m394.6    \u001b[0m | \u001b[0m0.1033   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-1.796   \u001b[0m | \u001b[0m5.239    \u001b[0m | \u001b[0m0.9086   \u001b[0m | \u001b[0m5.643    \u001b[0m | \u001b[0m143.9    \u001b[0m | \u001b[0m0.1301   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-1.539   \u001b[0m | \u001b[0m3.097    \u001b[0m | \u001b[0m0.6788   \u001b[0m | \u001b[0m4.905    \u001b[0m | \u001b[0m132.8    \u001b[0m | \u001b[0m0.4916   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-1.515   \u001b[0m | \u001b[0m3.267    \u001b[0m | \u001b[0m0.5741   \u001b[0m | \u001b[0m4.321    \u001b[0m | \u001b[0m294.7    \u001b[0m | \u001b[0m0.6998   \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-1.929   \u001b[0m | \u001b[0m3.512    \u001b[0m | \u001b[0m0.4141   \u001b[0m | \u001b[0m9.25     \u001b[0m | \u001b[0m207.1    \u001b[0m | \u001b[0m0.05005  \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-1.605   \u001b[0m | \u001b[0m5.679    \u001b[0m | \u001b[0m0.6638   \u001b[0m | \u001b[0m7.634    \u001b[0m | \u001b[0m472.3    \u001b[0m | \u001b[0m0.5866   \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-1.417   \u001b[0m | \u001b[0m7.517    \u001b[0m | \u001b[0m0.1375   \u001b[0m | \u001b[0m4.253    \u001b[0m | \u001b[0m403.7    \u001b[0m | \u001b[0m0.3977   \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-1.717   \u001b[0m | \u001b[0m3.827    \u001b[0m | \u001b[0m0.9275   \u001b[0m | \u001b[0m6.13     \u001b[0m | \u001b[0m375.4    \u001b[0m | \u001b[0m0.726    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-1.744   \u001b[0m | \u001b[0m7.417    \u001b[0m | \u001b[0m0.6237   \u001b[0m | \u001b[0m9.758    \u001b[0m | \u001b[0m174.4    \u001b[0m | \u001b[0m0.27     \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-1.508   \u001b[0m | \u001b[0m7.479    \u001b[0m | \u001b[0m0.4281   \u001b[0m | \u001b[0m11.68    \u001b[0m | \u001b[0m331.7    \u001b[0m | \u001b[0m0.6217   \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-1.742   \u001b[0m | \u001b[0m3.574    \u001b[0m | \u001b[0m0.9495   \u001b[0m | \u001b[0m7.049    \u001b[0m | \u001b[0m289.2    \u001b[0m | \u001b[0m0.4082   \u001b[0m |\n",
      "| \u001b[95m20       \u001b[0m | \u001b[95m-1.162   \u001b[0m | \u001b[95m4.185    \u001b[0m | \u001b[95m0.9034   \u001b[0m | \u001b[95m8.163    \u001b[0m | \u001b[95m1.435    \u001b[0m | \u001b[95m0.6172   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-1.727   \u001b[0m | \u001b[0m3.061    \u001b[0m | \u001b[0m0.9857   \u001b[0m | \u001b[0m4.228    \u001b[0m | \u001b[0m294.3    \u001b[0m | \u001b[0m0.3011   \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-1.564   \u001b[0m | \u001b[0m4.499    \u001b[0m | \u001b[0m0.4585   \u001b[0m | \u001b[0m8.275    \u001b[0m | \u001b[0m371.5    \u001b[0m | \u001b[0m0.4743   \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-1.581   \u001b[0m | \u001b[0m6.681    \u001b[0m | \u001b[0m0.6905   \u001b[0m | \u001b[0m3.863    \u001b[0m | \u001b[0m337.3    \u001b[0m | \u001b[0m0.6519   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-2.348   \u001b[0m | \u001b[0m4.942    \u001b[0m | \u001b[0m0.9258   \u001b[0m | \u001b[0m10.64    \u001b[0m | \u001b[0m293.3    \u001b[0m | \u001b[0m0.2195   \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-1.718   \u001b[0m | \u001b[0m3.404    \u001b[0m | \u001b[0m0.85     \u001b[0m | \u001b[0m10.21    \u001b[0m | \u001b[0m344.0    \u001b[0m | \u001b[0m0.756    \u001b[0m |\n",
      "| \u001b[95m26       \u001b[0m | \u001b[95m-1.09    \u001b[0m | \u001b[95m4.169    \u001b[0m | \u001b[95m0.1374   \u001b[0m | \u001b[95m10.96    \u001b[0m | \u001b[95m2.942    \u001b[0m | \u001b[95m0.7489   \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-1.141   \u001b[0m | \u001b[0m5.04     \u001b[0m | \u001b[0m0.5212   \u001b[0m | \u001b[0m9.174    \u001b[0m | \u001b[0m2.132    \u001b[0m | \u001b[0m0.4838   \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-1.27    \u001b[0m | \u001b[0m3.335    \u001b[0m | \u001b[0m0.8666   \u001b[0m | \u001b[0m9.2      \u001b[0m | \u001b[0m3.778    \u001b[0m | \u001b[0m0.8765   \u001b[0m |\n",
      "| \u001b[95m29       \u001b[0m | \u001b[95m-1.087   \u001b[0m | \u001b[95m3.375    \u001b[0m | \u001b[95m0.6334   \u001b[0m | \u001b[95m10.1     \u001b[0m | \u001b[95m0.2536   \u001b[0m | \u001b[95m0.1169   \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m6.423    \u001b[0m | \u001b[0m0.5278   \u001b[0m | \u001b[0m11.52    \u001b[0m | \u001b[0m0.5544   \u001b[0m | \u001b[0m0.8961   \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-1.173   \u001b[0m | \u001b[0m7.661    \u001b[0m | \u001b[0m0.5089   \u001b[0m | \u001b[0m11.35    \u001b[0m | \u001b[0m3.783    \u001b[0m | \u001b[0m0.3305   \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m7.744    \u001b[0m | \u001b[0m0.1453   \u001b[0m | \u001b[0m6.71     \u001b[0m | \u001b[0m0.2026   \u001b[0m | \u001b[0m0.145    \u001b[0m |\n",
      "| \u001b[95m33       \u001b[0m | \u001b[95m-1.086   \u001b[0m | \u001b[95m7.457    \u001b[0m | \u001b[95m0.01592  \u001b[0m | \u001b[95m3.256    \u001b[0m | \u001b[95m1.536    \u001b[0m | \u001b[95m0.09605  \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m3.864    \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.382    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m4.293    \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m4.761    \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-1.086   \u001b[0m | \u001b[0m5.037    \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m299.4    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-1.146   \u001b[0m | \u001b[0m7.418    \u001b[0m | \u001b[0m0.07317  \u001b[0m | \u001b[0m3.913    \u001b[0m | \u001b[0m7.936    \u001b[0m | \u001b[0m0.7905   \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m11.21    \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m7.364    \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m14.81    \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-1.502   \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.8      \u001b[0m | \u001b[0m16.67    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-1.086   \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.171    \u001b[0m | \u001b[0m304.9    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-1.086   \u001b[0m | \u001b[0m8.0      \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m305.1    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m-1.49    \u001b[0m | \u001b[0m5.63     \u001b[0m | \u001b[0m0.3891   \u001b[0m | \u001b[0m8.235    \u001b[0m | \u001b[0m307.2    \u001b[0m | \u001b[0m0.9234   \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-1.704   \u001b[0m | \u001b[0m7.354    \u001b[0m | \u001b[0m0.8228   \u001b[0m | \u001b[0m3.138    \u001b[0m | \u001b[0m311.3    \u001b[0m | \u001b[0m0.9827   \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-1.432   \u001b[0m | \u001b[0m7.942    \u001b[0m | \u001b[0m0.1443   \u001b[0m | \u001b[0m4.952    \u001b[0m | \u001b[0m301.7    \u001b[0m | \u001b[0m0.376    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-1.201   \u001b[0m | \u001b[0m7.952    \u001b[0m | \u001b[0m0.1962   \u001b[0m | \u001b[0m5.793    \u001b[0m | \u001b[0m12.16    \u001b[0m | \u001b[0m0.2699   \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-1.566   \u001b[0m | \u001b[0m7.5      \u001b[0m | \u001b[0m0.4841   \u001b[0m | \u001b[0m3.28     \u001b[0m | \u001b[0m93.97    \u001b[0m | \u001b[0m0.9509   \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-1.524   \u001b[0m | \u001b[0m3.824    \u001b[0m | \u001b[0m0.6848   \u001b[0m | \u001b[0m11.19    \u001b[0m | \u001b[0m82.56    \u001b[0m | \u001b[0m0.494    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m8.0      \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m191.1    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m-1.865   \u001b[0m | \u001b[0m3.433    \u001b[0m | \u001b[0m0.7908   \u001b[0m | \u001b[0m3.058    \u001b[0m | \u001b[0m189.2    \u001b[0m | \u001b[0m0.0105   \u001b[0m |\n",
      "=====================================================================================\n",
      "{'target': -1.0862585255271215, 'params': {'gamma': 7.457013829762214, 'learning_rate': 0.015924381489427538, 'max_depth': 3.2555064616370575, 'n_estimators': 1.5361839240111141, 'subsample': 0.09604730679045614}}\n"
     ]
    }
   ],
   "source": [
    "parambounds = {\n",
    "    'learning_rate': (0.00001, 1),\n",
    "    'n_estimators': (0, 500),\n",
    "    'max_depth': (3,12),\n",
    "    'subsample': (0.0001, 1.0),  \n",
    "    'gamma': (3, 8),\n",
    "    \n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=30,)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b8b096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "xgb_params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': int(params[\"max_depth\"]),\n",
    "    'learning_rate': params[\"learning_rate\"],\n",
    "    'n_estimators': int(params[\"n_estimators\"]),\n",
    "    'gamma': params[\"gamma\"],\n",
    "    'subsample': params[\"subsample\"]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "data_preprocessor = Prepare_data(column_list, [numeric_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf280501-8dc3-4496-a7c7-a29767594c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training RMSLE: 0.858, Validation RMSLE: 1.731\n",
      "Fold 2:\n",
      "Training RMSLE: 0.800, Validation RMSLE: 0.957\n",
      "Fold 3:\n",
      "Training RMSLE: 0.841, Validation RMSLE: 1.117\n",
      "Fold 4:\n",
      "Training RMSLE: 0.758, Validation RMSLE: 0.751\n",
      "Average Across Folds\n",
      "Training RMSLE:0.815, Validation RMSLE: 1.196\n"
     ]
    }
   ],
   "source": [
    "# Use time series split for cross validation. \n",
    "cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "\n",
    "# Create lists to append MSE scores. \n",
    "train_msle = []\n",
    "valid_msle = []\n",
    "\n",
    "# Dates to index through. \n",
    "dates = X_C.index.drop_duplicates()\n",
    "a = 0\n",
    "# Perform Cross-Validation to determine how model will do on unseen data.\n",
    "for train_index, valid_index in cv_split.split(dates):\n",
    "    a = a+1\n",
    "    print(f\"Fold {a}:\") \n",
    "    model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=True, to_tensor=False)\n",
    "    \n",
    "    # Index dates.\n",
    "    date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "    # Selecting data for y_train and y_valid.\n",
    "    y_train = y.loc[date_train]\n",
    "    y_valid = y.loc[date_valid]\n",
    "    \n",
    "    # Selecting data for X_train and X_valid.\n",
    "    X_train = X_C.loc[date_train]\n",
    "    X_valid = X_C.loc[date_valid]\n",
    "    \n",
    "    X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    X_train = X_train.set_index([\"date\"])\n",
    "    X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "    y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    y_train = y_train.set_index([\"date\"])\n",
    "    y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "    # Fitting model.\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Create predictions for Trainning and Validation.\n",
    "    fit = model.predict(X_train)\n",
    "    pred = model.predict(X_valid)\n",
    "    \n",
    "    # MSE for trainning and validation. \n",
    "    train_msle.append(float(mean_squared_log_error(y_train[\"sales\"], fit[\"sales\"])))\n",
    "    valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "    \n",
    "    print(f\"Training RMSLE: {cp.sqrt(mean_squared_log_error(y_train.sales, fit.sales)):.3f}, Validation RMSLE: {cp.sqrt(mean_squared_log_error(y_valid.sales, pred.sales)):.3f}\")\n",
    "\n",
    "# Returns the square root of the average of the MSE.\n",
    "print(\"Average Across Folds\")\n",
    "print(f\"Training RMSLE:{np.sqrt(np.mean(train_msle)):.3f}, Validation RMSLE: {np.sqrt(np.mean(valid_msle)):.3f}\")\n",
    "\n",
    "e_1 = 1/np.sqrt(np.mean(valid_msle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "376a415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=True, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "pred_1 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4175e",
   "metadata": {},
   "source": [
    "## Linear Regression, XGBoost, Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a619f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(n_estimators, gamma, subsample, max_depth, learning_rate):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    \n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    \n",
    "    params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': max_depth,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': n_estimators,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample}\n",
    "\n",
    "    model_2 = XGBRegressor(**params)\n",
    "    # Use time series split for cross validation. \n",
    "    cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "    \n",
    "    # Create lists to append MSLE scores.\n",
    "    valid_msle = []\n",
    "\n",
    "    # Dates to index through. \n",
    "    dates = X_C.index.drop_duplicates()\n",
    "\n",
    "    \n",
    "    # Perform Cross-Validation to determine how model will do on unseen data.\n",
    "    for train_index, valid_index in cv_split.split(dates):\n",
    "\n",
    "        model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=False, to_tensor=False)\n",
    "\n",
    "        # Index dates.\n",
    "        date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "        # Selecting data for y_train and y_valid.\n",
    "        y_train = y.loc[date_train]\n",
    "        y_valid = y.loc[date_valid]\n",
    "\n",
    "        # Selecting data for X_train and X_valid.\n",
    "        X_train = X_C.loc[date_train]\n",
    "        X_valid = X_C.loc[date_valid]\n",
    "\n",
    "        X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        X_train = X_train.set_index([\"date\"])\n",
    "        X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "        y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        y_train = y_train.set_index([\"date\"])\n",
    "        y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "        # Fitting model.\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Create predictions for Trainning and Validation.\n",
    "        pred = model.predict(X_valid)\n",
    "\n",
    "        # MSE for trainning and validation. \n",
    "        valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "\n",
    "\n",
    "    return -float(np.sqrt(np.mean(valid_msle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc1853d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-1.876   \u001b[0m | \u001b[0m5.085    \u001b[0m | \u001b[0m0.7203   \u001b[0m | \u001b[0m3.001    \u001b[0m | \u001b[0m151.2    \u001b[0m | \u001b[0m0.1468   \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-1.491   \u001b[0m | \u001b[95m3.462    \u001b[0m | \u001b[95m0.1863   \u001b[0m | \u001b[95m6.11     \u001b[0m | \u001b[95m198.4    \u001b[0m | \u001b[95m0.5389   \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-2.68    \u001b[0m | \u001b[0m5.096    \u001b[0m | \u001b[0m0.6852   \u001b[0m | \u001b[0m4.84     \u001b[0m | \u001b[0m439.1    \u001b[0m | \u001b[0m0.02748  \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-1.536   \u001b[0m | \u001b[0m6.352    \u001b[0m | \u001b[0m0.4173   \u001b[0m | \u001b[0m8.028    \u001b[0m | \u001b[0m70.19    \u001b[0m | \u001b[0m0.1982   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-1.631   \u001b[0m | \u001b[0m7.004    \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m5.821    \u001b[0m | \u001b[0m346.2    \u001b[0m | \u001b[0m0.8764   \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-2.068   \u001b[0m | \u001b[0m7.473    \u001b[0m | \u001b[0m0.08505  \u001b[0m | \u001b[0m3.351    \u001b[0m | \u001b[0m84.92    \u001b[0m | \u001b[0m0.8782   \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m-1.238   \u001b[0m | \u001b[95m3.492    \u001b[0m | \u001b[95m0.4211   \u001b[0m | \u001b[95m11.62    \u001b[0m | \u001b[95m266.6    \u001b[0m | \u001b[95m0.6919   \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-1.457   \u001b[0m | \u001b[0m4.578    \u001b[0m | \u001b[0m0.6865   \u001b[0m | \u001b[0m10.51    \u001b[0m | \u001b[0m9.144    \u001b[0m | \u001b[0m0.7502   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-1.96    \u001b[0m | \u001b[0m7.944    \u001b[0m | \u001b[0m0.7482   \u001b[0m | \u001b[0m5.524    \u001b[0m | \u001b[0m394.6    \u001b[0m | \u001b[0m0.1033   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-2.025   \u001b[0m | \u001b[0m5.239    \u001b[0m | \u001b[0m0.9086   \u001b[0m | \u001b[0m5.643    \u001b[0m | \u001b[0m143.9    \u001b[0m | \u001b[0m0.1301   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-1.667   \u001b[0m | \u001b[0m3.097    \u001b[0m | \u001b[0m0.6788   \u001b[0m | \u001b[0m4.905    \u001b[0m | \u001b[0m132.8    \u001b[0m | \u001b[0m0.4916   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-1.57    \u001b[0m | \u001b[0m3.267    \u001b[0m | \u001b[0m0.5741   \u001b[0m | \u001b[0m4.321    \u001b[0m | \u001b[0m294.7    \u001b[0m | \u001b[0m0.6998   \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-2.172   \u001b[0m | \u001b[0m3.512    \u001b[0m | \u001b[0m0.4141   \u001b[0m | \u001b[0m9.25     \u001b[0m | \u001b[0m207.1    \u001b[0m | \u001b[0m0.05005  \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-1.569   \u001b[0m | \u001b[0m5.679    \u001b[0m | \u001b[0m0.6638   \u001b[0m | \u001b[0m7.634    \u001b[0m | \u001b[0m472.3    \u001b[0m | \u001b[0m0.5866   \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-1.651   \u001b[0m | \u001b[0m7.517    \u001b[0m | \u001b[0m0.1375   \u001b[0m | \u001b[0m4.253    \u001b[0m | \u001b[0m403.7    \u001b[0m | \u001b[0m0.3977   \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-1.635   \u001b[0m | \u001b[0m3.827    \u001b[0m | \u001b[0m0.9275   \u001b[0m | \u001b[0m6.13     \u001b[0m | \u001b[0m375.4    \u001b[0m | \u001b[0m0.726    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-1.692   \u001b[0m | \u001b[0m7.417    \u001b[0m | \u001b[0m0.6237   \u001b[0m | \u001b[0m9.758    \u001b[0m | \u001b[0m174.4    \u001b[0m | \u001b[0m0.27     \u001b[0m |\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m-1.229   \u001b[0m | \u001b[95m7.479    \u001b[0m | \u001b[95m0.4281   \u001b[0m | \u001b[95m11.68    \u001b[0m | \u001b[95m331.7    \u001b[0m | \u001b[95m0.6217   \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-1.914   \u001b[0m | \u001b[0m3.574    \u001b[0m | \u001b[0m0.9495   \u001b[0m | \u001b[0m7.049    \u001b[0m | \u001b[0m289.2    \u001b[0m | \u001b[0m0.4082   \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-2.093   \u001b[0m | \u001b[0m4.185    \u001b[0m | \u001b[0m0.9034   \u001b[0m | \u001b[0m8.163    \u001b[0m | \u001b[0m1.435    \u001b[0m | \u001b[0m0.6172   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-1.56    \u001b[0m | \u001b[0m3.625    \u001b[0m | \u001b[0m0.7264   \u001b[0m | \u001b[0m6.388    \u001b[0m | \u001b[0m375.6    \u001b[0m | \u001b[0m0.8032   \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-1.45    \u001b[0m | \u001b[0m7.999    \u001b[0m | \u001b[0m0.1723   \u001b[0m | \u001b[0m9.641    \u001b[0m | \u001b[0m330.8    \u001b[0m | \u001b[0m0.1235   \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-1.233   \u001b[0m | \u001b[0m7.272    \u001b[0m | \u001b[0m0.5263   \u001b[0m | \u001b[0m11.14    \u001b[0m | \u001b[0m334.4    \u001b[0m | \u001b[0m0.7845   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-1.564   \u001b[0m | \u001b[0m4.044    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m12.0     \u001b[0m | \u001b[0m332.8    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[95m25       \u001b[0m | \u001b[95m-1.17    \u001b[0m | \u001b[95m3.69     \u001b[0m | \u001b[95m0.2674   \u001b[0m | \u001b[95m11.02    \u001b[0m | \u001b[95m263.1    \u001b[0m | \u001b[95m0.6616   \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-2.299   \u001b[0m | \u001b[0m6.488    \u001b[0m | \u001b[0m0.5443   \u001b[0m | \u001b[0m8.453    \u001b[0m | \u001b[0m265.2    \u001b[0m | \u001b[0m0.03979  \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-3.499   \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m12.0     \u001b[0m | \u001b[0m264.5    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-1.563   \u001b[0m | \u001b[0m6.307    \u001b[0m | \u001b[0m0.5043   \u001b[0m | \u001b[0m5.35     \u001b[0m | \u001b[0m311.2    \u001b[0m | \u001b[0m0.4069   \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-1.187   \u001b[0m | \u001b[0m3.704    \u001b[0m | \u001b[0m0.2668   \u001b[0m | \u001b[0m10.66    \u001b[0m | \u001b[0m224.5    \u001b[0m | \u001b[0m0.7105   \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-1.826   \u001b[0m | \u001b[0m5.345    \u001b[0m | \u001b[0m0.03766  \u001b[0m | \u001b[0m4.172    \u001b[0m | \u001b[0m352.8    \u001b[0m | \u001b[0m0.4685   \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-1.919   \u001b[0m | \u001b[0m4.923    \u001b[0m | \u001b[0m0.5323   \u001b[0m | \u001b[0m3.276    \u001b[0m | \u001b[0m66.82    \u001b[0m | \u001b[0m0.06187  \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-1.588   \u001b[0m | \u001b[0m4.341    \u001b[0m | \u001b[0m0.5404   \u001b[0m | \u001b[0m7.085    \u001b[0m | \u001b[0m488.8    \u001b[0m | \u001b[0m0.3141   \u001b[0m |\n",
      "| \u001b[95m33       \u001b[0m | \u001b[95m-1.167   \u001b[0m | \u001b[95m7.115    \u001b[0m | \u001b[95m0.1682   \u001b[0m | \u001b[95m11.39    \u001b[0m | \u001b[95m332.0    \u001b[0m | \u001b[95m0.3698   \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-1.345   \u001b[0m | \u001b[0m7.228    \u001b[0m | \u001b[0m0.4384   \u001b[0m | \u001b[0m10.85    \u001b[0m | \u001b[0m332.5    \u001b[0m | \u001b[0m0.5843   \u001b[0m |\n",
      "| \u001b[95m35       \u001b[0m | \u001b[95m-1.149   \u001b[0m | \u001b[95m3.286    \u001b[0m | \u001b[95m0.1771   \u001b[0m | \u001b[95m10.13    \u001b[0m | \u001b[95m224.0    \u001b[0m | \u001b[95m0.5787   \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-1.186   \u001b[0m | \u001b[0m7.603    \u001b[0m | \u001b[0m0.07108  \u001b[0m | \u001b[0m11.56    \u001b[0m | \u001b[0m332.8    \u001b[0m | \u001b[0m0.3791   \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-1.73    \u001b[0m | \u001b[0m3.865    \u001b[0m | \u001b[0m0.8643   \u001b[0m | \u001b[0m11.04    \u001b[0m | \u001b[0m262.3    \u001b[0m | \u001b[0m0.5093   \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-1.816   \u001b[0m | \u001b[0m3.761    \u001b[0m | \u001b[0m0.8491   \u001b[0m | \u001b[0m10.61    \u001b[0m | \u001b[0m223.8    \u001b[0m | \u001b[0m0.3811   \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-1.341   \u001b[0m | \u001b[0m3.382    \u001b[0m | \u001b[0m0.6263   \u001b[0m | \u001b[0m9.921    \u001b[0m | \u001b[0m224.9    \u001b[0m | \u001b[0m0.8779   \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-3.543   \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m1e-05    \u001b[0m | \u001b[0m10.42    \u001b[0m | \u001b[0m224.7    \u001b[0m | \u001b[0m0.2526   \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-1.34    \u001b[0m | \u001b[0m3.235    \u001b[0m | \u001b[0m0.2434   \u001b[0m | \u001b[0m7.173    \u001b[0m | \u001b[0m193.5    \u001b[0m | \u001b[0m0.9948   \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-2.132   \u001b[0m | \u001b[0m3.031    \u001b[0m | \u001b[0m0.5757   \u001b[0m | \u001b[0m11.39    \u001b[0m | \u001b[0m266.5    \u001b[0m | \u001b[0m0.1255   \u001b[0m |\n",
      "| \u001b[95m43       \u001b[0m | \u001b[95m-1.072   \u001b[0m | \u001b[95m3.982    \u001b[0m | \u001b[95m0.1944   \u001b[0m | \u001b[95m11.35    \u001b[0m | \u001b[95m263.2    \u001b[0m | \u001b[95m0.7835   \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-1.166   \u001b[0m | \u001b[0m3.863    \u001b[0m | \u001b[0m0.1082   \u001b[0m | \u001b[0m9.761    \u001b[0m | \u001b[0m350.6    \u001b[0m | \u001b[0m0.9778   \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-1.648   \u001b[0m | \u001b[0m4.154    \u001b[0m | \u001b[0m0.01379  \u001b[0m | \u001b[0m10.82    \u001b[0m | \u001b[0m263.2    \u001b[0m | \u001b[0m0.7883   \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-1.23    \u001b[0m | \u001b[0m6.826    \u001b[0m | \u001b[0m0.3246   \u001b[0m | \u001b[0m9.083    \u001b[0m | \u001b[0m185.3    \u001b[0m | \u001b[0m0.8337   \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-1.664   \u001b[0m | \u001b[0m3.551    \u001b[0m | \u001b[0m0.9603   \u001b[0m | \u001b[0m6.453    \u001b[0m | \u001b[0m375.2    \u001b[0m | \u001b[0m0.6267   \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-1.508   \u001b[0m | \u001b[0m5.611    \u001b[0m | \u001b[0m0.3667   \u001b[0m | \u001b[0m7.946    \u001b[0m | \u001b[0m472.4    \u001b[0m | \u001b[0m0.2987   \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-1.317   \u001b[0m | \u001b[0m6.934    \u001b[0m | \u001b[0m0.0918   \u001b[0m | \u001b[0m9.596    \u001b[0m | \u001b[0m180.9    \u001b[0m | \u001b[0m0.6571   \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m-1.61    \u001b[0m | \u001b[0m6.977    \u001b[0m | \u001b[0m0.996    \u001b[0m | \u001b[0m5.448    \u001b[0m | \u001b[0m345.8    \u001b[0m | \u001b[0m0.791    \u001b[0m |\n",
      "=====================================================================================\n",
      "{'target': -1.072175256013829, 'params': {'gamma': 3.982420624674882, 'learning_rate': 0.19443892207987015, 'max_depth': 11.349945478692394, 'n_estimators': 263.1868476620616, 'subsample': 0.7835133769716123}}\n"
     ]
    }
   ],
   "source": [
    "parambounds = {\n",
    "    'learning_rate': (0.00001, 1),\n",
    "    'n_estimators': (0, 500),\n",
    "    'max_depth': (3,12),\n",
    "    'subsample': (0.0001, 1.0),  \n",
    "    'gamma': (3, 8),\n",
    "    \n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=30,)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b943e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "xgb_params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': int(params[\"max_depth\"]),\n",
    "    'learning_rate': params[\"learning_rate\"],\n",
    "    'n_estimators': int(params[\"n_estimators\"]),\n",
    "    'gamma': params[\"gamma\"],\n",
    "    'subsample': params[\"subsample\"]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\n",
    "model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8e36bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training RMSLE: 0.872, Validation RMSLE: 1.025\n",
      "Fold 2:\n",
      "Training RMSLE: 0.929, Validation RMSLE: 1.059\n",
      "Fold 3:\n",
      "Training RMSLE: 0.931, Validation RMSLE: 1.229\n",
      "Fold 4:\n",
      "Training RMSLE: 0.942, Validation RMSLE: 0.957\n",
      "Average Across Folds\n",
      "Training RMSLE:0.919, Validation RMSLE: 1.072\n"
     ]
    }
   ],
   "source": [
    "# Use time series split for cross validation. \n",
    "cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "\n",
    "# Create lists to append MSE scores. \n",
    "train_msle = []\n",
    "valid_msle = []\n",
    "\n",
    "# Dates to index through. \n",
    "dates = X_C.index.drop_duplicates()\n",
    "a = 0\n",
    "# Perform Cross-Validation to determine how model will do on unseen data.\n",
    "for train_index, valid_index in cv_split.split(dates):\n",
    "    a = a+1\n",
    "    print(f\"Fold {a}:\") \n",
    "    model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=False, to_tensor=False)\n",
    "    \n",
    "    # Index dates.\n",
    "    date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "    # Selecting data for y_train and y_valid.\n",
    "    y_train = y.loc[date_train]\n",
    "    y_valid = y.loc[date_valid]\n",
    "    \n",
    "    # Selecting data for X_train and X_valid.\n",
    "    X_train = X_C.loc[date_train]\n",
    "    X_valid = X_C.loc[date_valid]\n",
    "    \n",
    "    X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    X_train = X_train.set_index([\"date\"])\n",
    "    X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "    y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "    y_train = y_train.set_index([\"date\"])\n",
    "    y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "    # Fitting model.\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Create predictions for Trainning and Validation.\n",
    "    fit = model.predict(X_train)\n",
    "    pred = model.predict(X_valid)\n",
    "    \n",
    "    # MSE for trainning and validation. \n",
    "    train_msle.append(float(mean_squared_log_error(y_train[\"sales\"], fit[\"sales\"])))\n",
    "    valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "    \n",
    "    print(f\"Training RMSLE: {cp.sqrt(mean_squared_log_error(y_train.sales, fit.sales)):.3f}, Validation RMSLE: {cp.sqrt(mean_squared_log_error(y_valid.sales, pred.sales)):.3f}\")\n",
    "\n",
    "# Returns the square root of the average of the MSE.\n",
    "print(\"Average Across Folds\")\n",
    "print(f\"Training RMSLE:{np.sqrt(np.mean(train_msle)):.3f}, Validation RMSLE: {np.sqrt(np.mean(valid_msle)):.3f}\")\n",
    "\n",
    "e_2 = 1/np.sqrt(np.mean(valid_msle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca58c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=True, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "pred_2 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd116d2-42fc-489e-8167-20406eb2c7d1",
   "metadata": {},
   "source": [
    "## Final Predictions and Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "161b6096-78e0-4052-b58a-398d9c2b54e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2724678</th>\n",
       "      <td>11.951799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726460</th>\n",
       "      <td>11.304447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728242</th>\n",
       "      <td>8.576170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730024</th>\n",
       "      <td>11.927396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731806</th>\n",
       "      <td>10.620213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sales\n",
       "id                \n",
       "2724678  11.951799\n",
       "2726460  11.304447\n",
       "2728242   8.576170\n",
       "2730024  11.927396\n",
       "2731806  10.620213"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Predictions\n",
    "e_sum = e_1+e_2\n",
    "\n",
    "ensembled_pred = pred_1*e_1/e_sum + pred_2*e_2/e_sum\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ca14a1b-1fff-4e6e-adec-26c42425112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_pred.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a61c910-4a78-462f-ad44-f0c8d196abc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 554k/554k [00:02<00:00, 229kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to Store Sales - Time Series Forecasting"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.competition_submit('submission.csv','1st API Submission','store-sales-time-series-forecasting')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (rapidsai)",
   "language": "python",
   "name": "rapidsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
