{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fdbfd5-d749-441e-9291-48c9e26dfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e042b-275d-4a9b-85d8-77d00104f47c",
   "metadata": {},
   "source": [
    "## Imports Needed to run Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e2ccd0-6df8-4ca5-96b1-58a118dd875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for data wrangling \n",
    "import cudf\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ba9ed7-f231-4876-b860-c28fae1a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing given data\n",
    "train = cudf.read_csv(\"./input/train.csv\", parse_dates=['date'])\n",
    "\n",
    "test = cudf.read_csv(\"./input/test.csv\", parse_dates=['date'])\n",
    "\n",
    "oil = cudf.read_csv(\"./input/oil.csv\", parse_dates=['date'])\n",
    "\n",
    "holiday = cudf.read_csv(\"./input/holidays_events.csv\")\n",
    "\n",
    "store = cudf.read_csv(\"./input/stores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db416fb9-d8e8-4b59-a7a9-9aae161d54fe",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f704e17-0daa-43ac-82b1-83c6b0c916e9",
   "metadata": {},
   "source": [
    "### Capturing Seasonal Holiday Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a05e50e-ddda-4e59-9fa9-b2f39bf7d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dates to datetime\n",
    "holiday[\"date\"] = cudf.to_datetime(holiday[\"date\"], format='%Y-%m-%d')\n",
    "holiday = holiday.set_index(\"date\")\n",
    "\n",
    "# Keeping only celbrated holidays\n",
    "holiday = holiday.loc[(holiday[\"transferred\"]!=True)].drop(\"transferred\", axis=1)\n",
    "holiday.loc[holiday[\"type\"]==\"Transfer\", \"type\"] = \"Holiday\"\n",
    "\n",
    "# Bridged days are day where there is no work\n",
    "bridge = holiday.loc[holiday[\"type\"]==\"Bridge\"]\n",
    "bridge[\"bridge\"] = True\n",
    "bridge = bridge[[\"bridge\"]]\n",
    "\n",
    "# Special events\n",
    "event = holiday.loc[holiday[\"type\"]==\"Event\"][[\"description\"]]\n",
    "\n",
    "# Keeping only holidays\n",
    "holiday = holiday.loc[holiday[\"type\"]==\"Holiday\"]\n",
    "\n",
    "# Holidays celerbated localy \n",
    "loc_hol = holiday.loc[holiday[\"locale\"]==\"Local\"][[\"locale_name\", \"description\"]]\n",
    "\n",
    "# Holidays celerbrated regionally\n",
    "reg_hol = holiday.loc[holiday[\"locale\"]==\"Regional\"][[\"locale_name\", \"description\"]]\n",
    "\n",
    "#Holidays celberbrated nationally\n",
    "nat_hol = holiday.loc[holiday[\"locale\"]==\"National\"][[\"description\"]]\n",
    "\n",
    "# Recording days Earthquake\n",
    "quake = event.loc[event[\"description\"].str.find(\"Terremoto Manabi\")!=-1]\n",
    "quake[\"time_since_quake\"] = cp.arange(1,len(quake.index)+1)\n",
    "quake.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "# Removing Earthquake and adding Sporting Events\n",
    "event = event.loc[event[\"description\"].str.find(\"Terremoto Manabi\")==-1]\n",
    "event.loc[event[\"description\"].str.find(\"futbol\")!=-1, \"description\"]= \"Sports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8714e35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bridge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-12-24</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-26</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-04</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bridge\n",
       "date              \n",
       "2012-12-24    True\n",
       "2012-12-31    True\n",
       "2014-12-26    True\n",
       "2015-01-02    True\n",
       "2016-11-04    True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f7c2d-63a5-4f01-bfcb-553480b34fd3",
   "metadata": {},
   "source": [
    "### Location Specific Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e484d5b1-1842-465e-ae7c-51474ccc5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper format\n",
    "train[\"store_nbr\"] = train[\"store_nbr\"].astype(int)\n",
    "\n",
    "# Merging\n",
    "X = train.merge(store, on=\"store_nbr\", how=\"left\")\n",
    "X.drop(\"cluster\", axis=1, inplace=True)\n",
    "\n",
    "# Converting dates to datetime\n",
    "X[\"date\"] = cudf.to_datetime(X[\"date\"], format='%Y-%m-%d')\n",
    "\n",
    "# Creating feature measuring the total in store promotions.\n",
    "total_other_promo_store = X[[\"date\", \"store_nbr\", \"onpromotion\"]].groupby(['date', 'store_nbr']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_promo_store = total_other_promo_store.rename(columns={'onpromotion': 'total_other_promo_store',})\n",
    "\n",
    "# Creating feature measuring the total promotions in each town for similar products.\n",
    "total_other_city_promo = X[[\"date\", \"onpromotion\", \"family\", \"city\"]].groupby(['date', 'city', 'family']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_city_promo = total_other_city_promo.rename(columns={'onpromotion': 'total_other_city_promo',})\n",
    "\n",
    "# Adding new features\n",
    "X = X.merge(total_other_promo_store, on=['date', 'store_nbr'], how=\"left\")\n",
    "X = X.merge(total_other_city_promo, on=['date', 'city', 'family'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4df2e7df-a73d-4a5d-88bb-04e2b33052b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper format\n",
    "store[\"store_nbr\"] = store[\"store_nbr\"].astype(int)\n",
    "test[\"store_nbr\"] = test[\"store_nbr\"].astype(int)\n",
    "\n",
    "# Merging\n",
    "X_test = test.merge(store, on=\"store_nbr\", how=\"left\")\n",
    "X_test.drop(\"cluster\", axis=1, inplace=True)\n",
    "\n",
    "# Converting dates to datetime\n",
    "X_test[\"date\"] = cudf.to_datetime(X_test[\"date\"], format='%Y-%m-%d')\n",
    "\n",
    "# Creating feature measuring the total in store promotions.\n",
    "total_other_promo_store = X_test[[\"date\", \"store_nbr\", \"onpromotion\"]].groupby(['date', 'store_nbr']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_promo_store = total_other_promo_store.rename(columns={'onpromotion': 'total_other_promo_store',})\n",
    "\n",
    "# Creating feature measuring the total promotions in each town for similar products.\n",
    "total_other_city_promo = X_test[[\"date\", \"onpromotion\", \"family\", \"city\"]].groupby(['date', 'city', 'family']).sum()[\"onpromotion\"].reset_index()\n",
    "total_other_city_promo = total_other_city_promo.rename(columns={'onpromotion': 'total_other_city_promo',})\n",
    "\n",
    "# Adding new features\n",
    "X_test = X_test.merge(total_other_promo_store, on=['date', 'store_nbr'], how=\"left\")\n",
    "X_test = X_test.merge(total_other_city_promo, on=['date', 'city', 'family'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d76ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.set_index(\"date\")\n",
    "X_test = X_test.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a203c43-19df-48ba-9c9c-f6b423a2dd40",
   "metadata": {},
   "source": [
    "### Merging with Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e576cca1-b3eb-455f-aed1-65a679476c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding national holidays\n",
    "X = X.merge(nat_hol, on=\"date\", how=\"left\")\n",
    "\n",
    "# Bridge days\n",
    "X = X.merge(bridge, on=\"date\", how=\"left\")\n",
    "\n",
    "# Adding local holdays\n",
    "X = X.merge(loc_hol, left_on=[\"date\", \"city\"],\n",
    "            right_on=[\"date\", \"locale_name\"],\n",
    "            suffixes=(None, '_l'), how=\"left\"\n",
    "           )\n",
    "X.drop(\"locale_name\", axis=1, inplace=True)\n",
    "\n",
    "# Adding regional holidays\n",
    "X = X.merge(reg_hol, left_on=[\"date\", \"state\"],\n",
    "            right_on=[\"date\", \"locale_name\"], \n",
    "            suffixes=(None, '_r'),how=\"left\"\n",
    "           )\n",
    "X.drop(\"locale_name\", axis=1, inplace=True)\n",
    "\n",
    "# True if holiday that Day\n",
    "X[\"holiday\"] = (((X[\"descriptionNone\"].isnull()==False) | (X[\"description_l\"].isnull()==False)) | (X[\"description\"].isnull()==False))\n",
    "\n",
    "X[\"holiday_description\"] = X['descriptionNone'].fillna('') + X['description_l'].fillna('') + X['description'].fillna('')\n",
    "\n",
    "# Combine Holiday descriptions\n",
    "X.drop(\"descriptionNone\", axis=1, inplace=True)\n",
    "X.drop(\"description_l\", axis=1, inplace=True)\n",
    "X.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "#Events\n",
    "X = X.merge(event, on=\"date\", how=\"left\")\n",
    "X = X.rename(columns={'description': 'event',})\n",
    "X[\"event\"] = X[\"event\"].fillna(\"none\")\n",
    "\n",
    "# Adding Quake data\n",
    "X = X.merge(quake, on=\"date\", how=\"left\")\n",
    "X[\"time_since_quake\"] = X[\"time_since_quake\"].fillna(0)\n",
    "\n",
    "#To model a diminishing marginal effect on the economy by the earthquake\n",
    "X[\"time_since_quake_sq\"] = X[\"time_since_quake\"]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60cf2ae-a8ab-49ee-9b2c-354f2c34131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding national holidays\n",
    "X_test = X_test.merge(nat_hol, on=\"date\", how=\"left\")\n",
    "del nat_hol\n",
    "\n",
    "# Bridge days\n",
    "X_test = X_test.merge(bridge, on=\"date\", how=\"left\")\n",
    "del bridge\n",
    "\n",
    "# Adding local holdays\n",
    "X_test = X_test.merge(loc_hol, left_on=[\"date\", \"city\"],\n",
    "            right_on=[\"date\", \"locale_name\"],\n",
    "            suffixes=(None, '_l'), how=\"left\"\n",
    "           )\n",
    "X_test.drop(\"locale_name\", axis=1, inplace=True)\n",
    "del loc_hol\n",
    "\n",
    "# Adding regional holidays\n",
    "X_test = X_test.merge(reg_hol, left_on=[\"date\", \"state\"],\n",
    "            right_on=[\"date\", \"locale_name\"], \n",
    "            suffixes=(None, '_r'),how=\"left\"\n",
    "           )\n",
    "X_test.drop(\"locale_name\", axis=1, inplace=True)\n",
    "del reg_hol\n",
    "\n",
    "# True if holiday that Day\n",
    "X_test[\"holiday\"] = (((X_test[\"descriptionNone\"].isnull()==False) | (X_test[\"description_l\"].isnull()==False)) | (X_test[\"description\"].isnull()==False))\n",
    "\n",
    "X_test[\"holiday_description\"] = X_test['descriptionNone'].fillna('') + X_test['description_l'].fillna('') + X_test['description'].fillna('')\n",
    "\n",
    "# Combine Holiday descriptions\n",
    "X_test.drop(\"descriptionNone\", axis=1, inplace=True)\n",
    "X_test.drop(\"description_l\", axis=1, inplace=True)\n",
    "X_test.drop(\"description\", axis=1, inplace=True)\n",
    "\n",
    "#Events\n",
    "X_test = X_test.merge(event, on=\"date\", how=\"left\")\n",
    "X_test = X_test.rename(columns={'description': 'event',})\n",
    "X_test[\"event\"] = X_test[\"event\"].fillna(\"none\")\n",
    "del event\n",
    "\n",
    "# Adding Quake data\n",
    "X_test = X_test.merge(quake, on=\"date\", how=\"left\")\n",
    "X_test[\"time_since_quake\"] = X_test[\"time_since_quake\"].fillna(0)\n",
    "del quake\n",
    "\n",
    "#To model a diminishing marginal effect on the economy by the earthquake\n",
    "X_test[\"time_since_quake_sq\"] = X_test[\"time_since_quake\"]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059c7b0-ce19-4011-929d-cd39e25682df",
   "metadata": {},
   "source": [
    "### Merging with Oil Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d6f820-485e-4f44-a3ef-821638b194ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil[\"date\"] = cudf.to_datetime(oil[\"date\"], format='%Y-%m-%d')\n",
    "oil = oil.set_index(\"date\")\n",
    "X = X.merge(oil, on=\"date\", how=\"left\")\n",
    "X_test = X_test.merge(oil, on=\"date\", how=\"left\")\n",
    "\n",
    "del oil\n",
    "\n",
    "# There is no price of oil on days that the market is closed so we interpolate to get next value.\n",
    "X[\"dcoilwtico\"]= X[\"dcoilwtico\"].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "X_test[\"dcoilwtico\"]= X_test[\"dcoilwtico\"].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "\n",
    "# I just to do a rolling average to smooth out any problems with the empty values,\n",
    "# and to capture any effect of changes. \n",
    "X[\"dcoilwtico\"] = X[\"dcoilwtico\"].rolling(\n",
    "    window=30,       \n",
    "    min_periods=1,  \n",
    ").mean()\n",
    "\n",
    "X_test[\"dcoilwtico\"] = X_test[\"dcoilwtico\"].rolling(\n",
    "    window=30,       \n",
    "    min_periods=1,  \n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5cddac-bc83-4fce-903a-ce35d9675dcc",
   "metadata": {},
   "source": [
    "### Time Based Varriables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ef2ed5c-e767-476d-b9cc-50d06717e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variables\n",
    "X[\"day\"] = X.index.dayofweek\n",
    "X[\"dayofyear\"] = X.index.dayofyear\n",
    "X[\"month\"] = X.index.month\n",
    "X[\"year\"] = X.index.year\n",
    "\n",
    "# This varible says whether it is a workday.\n",
    "X[\"workday\"] = (((X.bridge.isnull()) & (X.holiday==False)) & ((X[\"day\"]!=5) & (X[\"day\"]!=6)))\n",
    "X.drop(\"bridge\", axis=1, inplace=True)\n",
    "\n",
    "# In Ecudor, people get paid on the 15 and the last day of the month\n",
    "X[\"payday\"] = ((X.index.day==15) | (X.index.day==X.index.to_series().dt.days_in_month)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7d9e6e-6b89-4dfd-b375-aa0a56cd1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time variables\n",
    "X_test[\"day\"] = X_test.index.dayofweek\n",
    "X_test[\"dayofyear\"] =X_test.index.dayofyear\n",
    "X_test[\"month\"] = X_test.index.month\n",
    "X_test[\"year\"] = X_test.index.year\n",
    "\n",
    "# This varible says whether it is a workday.\n",
    "X_test[\"workday\"] = (((X_test.bridge.isnull()) & (X_test.holiday==False)) & ((X_test[\"day\"]!=5) & (X_test[\"day\"]!=6)))\n",
    "X_test.drop(\"bridge\", axis=1, inplace=True)\n",
    "\n",
    "# In Ecudor, people get paid on the 15 and the last day of the month\n",
    "X_test[\"payday\"] = ((X_test.index.day==15) | (X_test.index.day==X_test.index.to_series().dt.days_in_month)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9d618-c5d0-4c41-8d31-e255fdba0d75",
   "metadata": {},
   "source": [
    "### Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2deef0a-75d4-4134-9c4e-1f0fb03537c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing data type\n",
    "X_test = X_test.reset_index()\n",
    "X_test = X_test.set_index(\"date\")\n",
    "\n",
    "X_test[\"onpromotion\"] = X_test[\"onpromotion\"].astype('float')\n",
    "X_test[\"total_other_promo_store\"] = X_test[\"total_other_promo_store\"].astype('float')\n",
    "X_test[\"total_other_city_promo\"] = X_test[\"total_other_city_promo\"].astype('float')\n",
    "X_test[\"holiday\"] = X_test[\"holiday\"].astype('float')\n",
    "\n",
    "X_test[\"family\"] = X_test[\"family\"].astype('category')\n",
    "X_test[\"store_nbr\"] = X_test[\"store_nbr\"].astype('category')\n",
    "X_test[\"holiday\"] = X_test[\"holiday\"].astype('category')\n",
    "X_test[\"event\"] = X_test[\"event\"].astype('category')\n",
    "X_test[\"city\"] = X_test[\"city\"].astype('category')\n",
    "X_test[\"state\"] = X_test[\"state\"].astype('category')\n",
    "X_test[\"type\"] = X_test[\"type\"].astype('category')\n",
    "X_test[\"workday\"] = X_test[\"workday\"].astype('category')\n",
    "X_test[\"payday\"] = X_test[\"payday\"].astype('category')\n",
    "X_test[\"holiday_description\"] = X_test[\"holiday_description\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "968334b4-6d18-4dd3-b646-d25286f5cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reset_index()\n",
    "X = X.set_index(\"date\")\n",
    "\n",
    "X[\"onpromotion\"] = X[\"onpromotion\"].astype('float')\n",
    "X[\"total_other_promo_store\"] = X[\"total_other_promo_store\"].astype('float')\n",
    "X[\"total_other_city_promo\"] = X[\"total_other_city_promo\"].astype('float')\n",
    "X[\"holiday\"] = X[\"holiday\"].astype('float')\n",
    "\n",
    "X[\"family\"] = X[\"family\"].astype('category')\n",
    "X[\"store_nbr\"] = X[\"store_nbr\"].astype('category')\n",
    "X[\"holiday\"] = X[\"holiday\"].astype('category')\n",
    "X[\"event\"] = X[\"event\"].astype('category')\n",
    "X[\"city\"] = X[\"city\"].astype('category')\n",
    "X[\"state\"] = X[\"state\"].astype('category')\n",
    "X[\"type\"] = X[\"type\"].astype('category')\n",
    "X[\"workday\"] = X[\"workday\"].astype('category')\n",
    "X[\"payday\"] = X[\"payday\"].astype('category')\n",
    "X[\"holiday_description\"] = X[\"holiday_description\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee883d",
   "metadata": {},
   "source": [
    "### Lagged Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc941ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lags(data, column, lags):\n",
    "    '''Takes Data and creates lagged features for every catergory'''\n",
    "    for k in range(1, lags+1):\n",
    "        data[f\"{column}_lag_{k}\"] = data.groupby([\"store_nbr\", \"family\"])[column].shift(k)\n",
    "\n",
    "def make_one_year_lag(data, column):\n",
    "    '''Takes Data and retrieves the values from the previous year'''\n",
    "    data[f\"{column}_one_year_lag\"] = data.groupby([\"store_nbr\", \"family\", \"dayofyear\"])[column].shift(1)\n",
    "    \n",
    "    # Any after a year is just the result of the store being closed\n",
    "    data[f\"{column}_one_year_lag\"] = data[f\"{column}_one_year_lag\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddb3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lag = cudf.concat([X[[\"store_nbr\", \"family\", \"dayofyear\", \"onpromotion\", \"dcoilwtico\", \"sales\"]], X_test[[\"store_nbr\", \"family\", \"onpromotion\", \"dcoilwtico\"]]], axis=0)\n",
    "X_lag = X_lag.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "\n",
    "X_lag[\"dayofyear\"] = X_lag.index.dayofyear\n",
    "        \n",
    "make_lags(X_lag, \"onpromotion\", 7)\n",
    "make_lags(X_lag, \"dcoilwtico\", 7)\n",
    "\n",
    "make_one_year_lag(X_lag, \"sales\")\n",
    "\n",
    "X_lag = X_lag.drop([\"dayofyear\", \"onpromotion\", \"dcoilwtico\", \"sales\"], axis=1)\n",
    "\n",
    "X = X.merge(X_lag, on=[\"date\", \"store_nbr\", \"family\"], how=\"left\")\n",
    "X_test = X_test.merge(X_lag, on=[\"date\", \"store_nbr\", \"family\"], how=\"left\")\n",
    "\n",
    "del X_lag\n",
    "\n",
    "X[\"Change_in_oil_prices\"] = X[\"dcoilwtico\"]-X[\"dcoilwtico_lag_1\"]\n",
    "X_test[\"Change_in_oil_prices\"] = X_test[\"dcoilwtico\"]-X_test[\"dcoilwtico_lag_1\"]\n",
    "X[\"Change_in_oil_prices\"] = X[\"Change_in_oil_prices\"].astype('float')\n",
    "X_test[\"Change_in_oil_prices\"] = X_test[\"Change_in_oil_prices\"].astype('float')\n",
    "\n",
    "X[\"promo_last_7_days\"] = X[X.columns[X.columns.str.find(\"onpromotion_lag\")==0]].sum(axis=1)\n",
    "X_test[\"promo_last_7_days\"] = X_test[X_test.columns[X_test.columns.str.find(\"onpromotion_lag\")==0]].sum(axis=1)\n",
    "X[\"promo_last_7_days\"] = X[\"promo_last_7_days\"].astype('float')\n",
    "X_test[\"promo_last_7_days\"] = X_test[\"promo_last_7_days\"].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e7aad",
   "metadata": {},
   "source": [
    "### Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc8b2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>total_other_promo_store</th>\n",
       "      <th>total_other_city_promo</th>\n",
       "      <th>holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>dcoilwtico_lag_1</th>\n",
       "      <th>dcoilwtico_lag_2</th>\n",
       "      <th>dcoilwtico_lag_3</th>\n",
       "      <th>dcoilwtico_lag_4</th>\n",
       "      <th>dcoilwtico_lag_5</th>\n",
       "      <th>dcoilwtico_lag_6</th>\n",
       "      <th>dcoilwtico_lag_7</th>\n",
       "      <th>sales_one_year_lag</th>\n",
       "      <th>Change_in_oil_prices</th>\n",
       "      <th>promo_last_7_days</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-10</th>\n",
       "      <td>17490</td>\n",
       "      <td>5</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.146207</td>\n",
       "      <td>93.190383</td>\n",
       "      <td>93.084</td>\n",
       "      <td>93.184325</td>\n",
       "      <td>93.363801</td>\n",
       "      <td>93.439028</td>\n",
       "      <td>93.162527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364793</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-10</th>\n",
       "      <td>17491</td>\n",
       "      <td>5</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.144000</td>\n",
       "      <td>93.192838</td>\n",
       "      <td>93.088</td>\n",
       "      <td>93.184348</td>\n",
       "      <td>93.393222</td>\n",
       "      <td>93.417803</td>\n",
       "      <td>93.154527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-10</th>\n",
       "      <td>17492</td>\n",
       "      <td>5</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.140000</td>\n",
       "      <td>93.195292</td>\n",
       "      <td>93.092</td>\n",
       "      <td>93.184370</td>\n",
       "      <td>93.393232</td>\n",
       "      <td>93.496667</td>\n",
       "      <td>93.146527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-10</th>\n",
       "      <td>17493</td>\n",
       "      <td>5</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.136000</td>\n",
       "      <td>93.210000</td>\n",
       "      <td>93.096</td>\n",
       "      <td>93.184393</td>\n",
       "      <td>93.393242</td>\n",
       "      <td>93.496000</td>\n",
       "      <td>93.138527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-10</th>\n",
       "      <td>17494</td>\n",
       "      <td>5</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.191429</td>\n",
       "      <td>93.197745</td>\n",
       "      <td>93.100</td>\n",
       "      <td>93.184415</td>\n",
       "      <td>93.393253</td>\n",
       "      <td>93.495333</td>\n",
       "      <td>93.130527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id store_nbr      family  onpromotion           city  \\\n",
       "date                                                                  \n",
       "2013-01-10  17490         5  AUTOMOTIVE          0.0  Santo Domingo   \n",
       "2013-01-10  17491         5   BABY CARE          0.0  Santo Domingo   \n",
       "2013-01-10  17492         5      BEAUTY          0.0  Santo Domingo   \n",
       "2013-01-10  17493         5   BEVERAGES          0.0  Santo Domingo   \n",
       "2013-01-10  17494         5       BOOKS          0.0  Santo Domingo   \n",
       "\n",
       "                                     state type  total_other_promo_store  \\\n",
       "date                                                                       \n",
       "2013-01-10  Santo Domingo de los Tsachilas    D                      0.0   \n",
       "2013-01-10  Santo Domingo de los Tsachilas    D                      0.0   \n",
       "2013-01-10  Santo Domingo de los Tsachilas    D                      0.0   \n",
       "2013-01-10  Santo Domingo de los Tsachilas    D                      0.0   \n",
       "2013-01-10  Santo Domingo de los Tsachilas    D                      0.0   \n",
       "\n",
       "            total_other_city_promo holiday  ... dcoilwtico_lag_1  \\\n",
       "date                                        ...                    \n",
       "2013-01-10                     0.0     0.0  ...        93.146207   \n",
       "2013-01-10                     0.0     0.0  ...        93.144000   \n",
       "2013-01-10                     0.0     0.0  ...        93.140000   \n",
       "2013-01-10                     0.0     0.0  ...        93.136000   \n",
       "2013-01-10                     0.0     0.0  ...        93.191429   \n",
       "\n",
       "            dcoilwtico_lag_2  dcoilwtico_lag_3  dcoilwtico_lag_4  \\\n",
       "date                                                               \n",
       "2013-01-10         93.190383            93.084         93.184325   \n",
       "2013-01-10         93.192838            93.088         93.184348   \n",
       "2013-01-10         93.195292            93.092         93.184370   \n",
       "2013-01-10         93.210000            93.096         93.184393   \n",
       "2013-01-10         93.197745            93.100         93.184415   \n",
       "\n",
       "            dcoilwtico_lag_5  dcoilwtico_lag_6  dcoilwtico_lag_7  \\\n",
       "date                                                               \n",
       "2013-01-10         93.363801         93.439028         93.162527   \n",
       "2013-01-10         93.393222         93.417803         93.154527   \n",
       "2013-01-10         93.393232         93.496667         93.146527   \n",
       "2013-01-10         93.393242         93.496000         93.138527   \n",
       "2013-01-10         93.393253         93.495333         93.130527   \n",
       "\n",
       "            sales_one_year_lag  Change_in_oil_prices  promo_last_7_days  \n",
       "date                                                                     \n",
       "2013-01-10                 0.0              0.364793                0.0  \n",
       "2013-01-10                 0.0              0.390000                0.0  \n",
       "2013-01-10                 0.0              0.417000                0.0  \n",
       "2013-01-10                 0.0              0.444000                0.0  \n",
       "2013-01-10                 0.0              0.411571                0.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X[[\"store_nbr\", \"family\", \"sales\"]]\n",
    "X.drop(\"sales\", axis=1, inplace=True)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a037ac91-c2d5-4aa7-af0e-875f1a941da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing early time with NaNs\n",
    "X = X.loc[X.index >= \"2015-07-01\"]\n",
    "y = y.loc[y.index >= \"2015-07-01\"]\n",
    "\n",
    "# X = X.loc[X.index >= \"2016-01-01\"]\n",
    "# y = y.loc[y.index >= \"2016-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705516e1-83e3-42c1-a5be-f4de5165a39f",
   "metadata": {},
   "source": [
    "## Trainning Model\n",
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50754687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing \n",
    "from cuml.dask.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from cuml.preprocessing import MinMaxScaler, StandardScaler, SimpleImputer, LabelEncoder, OneHotEncoder\n",
    "from cuml.compose import make_column_transformer\n",
    "from statsmodels.tsa.deterministic import CalendarFourier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cross-Validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from cuml.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from cuml.neighbors import KNeighborsRegressor\n",
    "from cuml.ensemble import RandomForestRegressor\n",
    "from cuml.metrics import mean_squared_error, mean_squared_log_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from data_wrangling import Prepare_data\n",
    "from hybrid_timeseries import Hybrid_Pipeline\n",
    "\n",
    "import warnings\n",
    "\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "# Ignore WarningS\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f29f4",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef4cf620-0ef7-4ab0-b3df-f3df15b17564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPUs detected: 1\n",
      "\n",
      "Device 0: NVIDIA GeForce RTX 4070\n",
      "  Total memory: 12.878086144 GB\n",
      "  CUDA Capability: 8.9\n",
      "  MultiProcessor Count: 46\n",
      "  Performing computation on Device 0...\n",
      "\n",
      "Summary of Results:\n",
      "Result from GPU 0: 250596928.00\n",
      "\n",
      "Average result from all GPUs: 250596928.00\n"
     ]
    }
   ],
   "source": [
    "# Function to perform a basic operation on the GPU and return the result\n",
    "def basic_gpu_operation(device):\n",
    "    # Create a random tensor of size (1000, 1000) on the specified device\n",
    "    x = torch.rand((1000, 1000), device=device)\n",
    "    # Perform a basic arithmetic operation (e.g., matrix multiplication with its transpose)\n",
    "    result = torch.matmul(x, x.t())\n",
    "    # Return the sum of the result to ensure a scalar value is returned\n",
    "    return result.sum()\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Print the total number of GPUs detected\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f'Total GPUs detected: {gpu_count}\\n')\n",
    "    # Initialize a list to hold the results from each GPU\n",
    "    results = []\n",
    "    # Loop through all available GPUs, print their properties, perform computations, and gather results\n",
    "    for i in range(gpu_count):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"Device {i}: {gpu_properties.name}\")\n",
    "        print(f\"  Total memory: {gpu_properties.total_memory / 1e9} GB\")\n",
    "        print(f\"  CUDA Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "        print(f\"  MultiProcessor Count: {gpu_properties.multi_processor_count}\")\n",
    "        print(f'  Performing computation on Device {i}...\\n')\n",
    "        # Perform the basic operation on the GPU and append the result to the results list\n",
    "        result = basic_gpu_operation(device)\n",
    "        results.append(result.item())  # Convert to Python number and append\n",
    "    # Summarize and print the results from each GPU\n",
    "    print('Summary of Results:')\n",
    "    for i, result in enumerate(results):\n",
    "        print(f'Result from GPU {i}: {result:.2f}')\n",
    "    # Perform some aggregation on the CPU (e.g., compute the average of all results)\n",
    "    results = cp.array(results)\n",
    "    average_result = cp.mean(results)\n",
    "    print(f'\\nAverage result from all GPUs: {average_result:.2f}')\n",
    "    # Optionally, provide a summary of overall GPU utilization or performance here\n",
    "    # This could involve more detailed metrics based on your specific use case or application\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "465ff6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse=nn.MSELoss().to(device)\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred+1), torch.log(actual + 1))\n",
    "    \n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_layer, n_hidden_1, n_hidden_2, drop):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.input_layer = input_layer\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        \n",
    "        # Layers: Linear, LSTM, Linear\n",
    "        self.linear1 = nn.Linear(input_layer, n_hidden_1)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.lstm = nn.LSTM(n_hidden_1, n_hidden_2, batch_first=True)\n",
    "        self.linear2 = nn.Linear(n_hidden_2, 1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output, (h_t, c_t) = self.lstm(x)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.ReLU(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LSTMRegressor():\n",
    "    def __init__(self, n_hidden=50, n_hidden_2=20, drop=0.2, epochs=100, early_stop=5, lr=0.01, Boosted=False, verbose=False):\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.drop = drop\n",
    "        if Boosted:\n",
    "            self.criterion = nn.MSELoss().to(device)\n",
    "        else: \n",
    "            self.criterion = MSLELoss()\n",
    "            \n",
    "        self.early_stop = early_stop \n",
    "        self.epochs = epochs \n",
    "        self.lr = lr\n",
    "        self.min_val_loss = float('inf')\n",
    "        self.min_val_loss_2 = float('inf')\n",
    "        self.verbose = verbose\n",
    "    def train(self, train_loader):\n",
    "        \n",
    "        self.model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch, y_batch\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(x_batch)\n",
    "            loss = self.criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    def pred(self, test_loader, valid=False, epoch=0):\n",
    "        \n",
    "        self.model.eval()\n",
    "        if valid:\n",
    "             \n",
    "            val_losses = 0\n",
    "            num = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in test_loader:\n",
    "                    x_batch = x_batch\n",
    "                    outputs = self.model(x_batch)\n",
    "\n",
    "                    loss = self.criterion(outputs, y_batch)\n",
    "                    val_losses=+loss.item()\n",
    "\n",
    "                    num=+1\n",
    "\n",
    "            val_loss = val_losses/num\n",
    "\n",
    "            if val_loss<self.min_val_loss:\n",
    "            \n",
    "                self.min_val_loss = val_loss\n",
    "                self.early_stop_count = 0\n",
    "            else:\n",
    "                self.early_stop_count+=1\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Validation score of {np.sqrt(val_loss):.4f}\")\n",
    "            if self.early_stop_count>=self.early_stop:\n",
    "                if self.verbose:\n",
    "                    print(f\"early stopping at Validation Score of {np.sqrt(self.min_val_loss):.4f}\")\n",
    "                    print()\n",
    "                self.stop = True\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions = []\n",
    "                for x_batch in test_loader:\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    outputs = self.model(x_batch)\n",
    "                    predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "                return np.concatenate(predictions)\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \n",
    "        if isinstance(X, list):\n",
    "            X_train, y_train = X[0], y[0]\n",
    "            self.model = LSTMModel(X_train.shape[1], n_hidden_1=self.n_hidden, n_hidden_2= self.n_hidden_2, drop=self.drop).to(device)\n",
    "            \n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "            train_loader = DataLoader(TensorDataset(X_train.to(device), y_train.to(device)), batch_size=31, shuffle=False)\n",
    "            \n",
    "            X_valid, y_valid = X[1], y[1]\n",
    "            test_loader = DataLoader(TensorDataset(X_valid.to(device), y_valid.to(device)), batch_size=31, shuffle=False)\n",
    "            \n",
    "            self.stop=False\n",
    "            self.early_stop_count =0 \n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                self.train(train_loader)\n",
    "                \n",
    "                self.pred(test_loader, valid=True, epoch=epoch)\n",
    "                if self.stop:\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "                X_train, y_train = X, y\n",
    "                self.model = LSTMModel(X_train.shape[1], n_hidden_1=self.n_hidden, n_hidden_2= self.n_hidden_2, drop=self.drop).to(device)\n",
    "                \n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "                train_loader = DataLoader(TensorDataset(X_train.to(device), y_train.to(device)), batch_size=31, shuffle=False)\n",
    "                \n",
    "                for epoch in range(self.epochs):\n",
    "                    self.train(train_loader)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        test_loader = DataLoader(X.to(device), batch_size=31, shuffle=False)\n",
    "        \n",
    "        outputs = self.pred(test_loader)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd09e7",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002a6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing steps\n",
    "numeric_transformer = [\"float\", StandardScaler()]\n",
    "categorical_transformer = [\"category\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "\n",
    "column_list = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "\n",
    "#data_preprocessor = Prepare_data(column_list, [numeric_transformer, categorical_transformer])\n",
    "data_preprocessor = Prepare_data(column_list, [numeric_transformer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39912e5",
   "metadata": {},
   "source": [
    "## Linear Regression, XGBoost, Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5a6e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = X.copy()\n",
    "\n",
    "X_C = X_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "\n",
    "X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "# X_C = X_C.set_index([\"date\"])\n",
    "# y = y.set_index([\"date\"])\n",
    "\n",
    "X_test_C = X_test.copy()\n",
    "X_test_C = X_test_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_test_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "#.drop([\"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1)\n",
    "X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be920998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_importance(X, y, model):\n",
    "    \n",
    "#     dates = X.index.drop_duplicates()\n",
    "    \n",
    "#     X_train = X.loc[dates[int(len(dates)*2/3):]]\n",
    "#     X_valid = X.loc[dates[:int(len(dates)*2/3)]]\n",
    "    \n",
    "#     y_train = y.loc[dates[int(len(dates)*2/3):]]\n",
    "#     y_valid = y.loc[dates[:int(len(dates)*2/3)]]\n",
    "    \n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     del X_train\n",
    "#     del y_train\n",
    "    \n",
    "#     pred = model.predict(X_valid)\n",
    "#     baseline = float(np.sqrt(mean_squared_log_error(y_valid.sales, pred.sales)))\n",
    "#     importance_dict = {}\n",
    "    \n",
    "#     for i in range(1, len(X_valid.columns)):\n",
    "        \n",
    "#         name = X_valid.columns[i]\n",
    "#         X_shuffle = X_valid.copy()\n",
    "#         X_shuffle = X_shuffle.to_pandas()\n",
    "#         X_shuffle[name] = X_shuffle[name].values[np.random.permutation(len(X_shuffle))]\n",
    "        \n",
    "#         X_shuffle = cudf.from_pandas(X_shuffle)\n",
    "#         pred = model.predict(X_shuffle)\n",
    "        \n",
    "#         del X_shuffle\n",
    "        \n",
    "#         importance_dict[name] = float(np.sqrt(mean_squared_log_error(y_valid.sales, pred.sales))) - baseline\n",
    "        \n",
    "        \n",
    "#     return importance_dict\n",
    "        \n",
    "    \n",
    "# xgb_params = {\n",
    "#     'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "#     'predictor': 'gpu_predictor',\n",
    "#     'enable_categorical': True,\n",
    "# }\n",
    "\n",
    "# xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\n",
    "# lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "# model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=True, to_tensor=False)\n",
    "\n",
    "# feature_df = pd.DataFrame.from_dict(feature_importance(X_C, y, model), orient='index', columns=['Change in MSE']).reset_index().sort_values(\"Change in MSE\")\n",
    "# feature_df = feature_df.rename({\"index\": \"Columns\"}, axis=1)\n",
    "# feature_df.plot.barh(x='Columns', y='Change in MSE', title='Feature Importance', color='blue')\n",
    "\n",
    "#X_C = X_C[feature_df.loc[feature_df[\"Change in MSE\"]>=0][\"Columns\"].append(pd.Series([\"store_nbr\", \"family\", \"id\"]), ignore_index=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b3c2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time_Series_CV(model, X_C, y, splits=4, verbose=False):\n",
    "    # Use time series split for cross validation. \n",
    "    cv_split = TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Create lists to append MSLE scores.\n",
    "    valid_msle = []\n",
    "    train_msle = []\n",
    "    \n",
    "    # Dates to index through. \n",
    "    dates = X_C.index.drop_duplicates()\n",
    "    a = 0\n",
    "    \n",
    "    # Perform Cross-Validation to determine how model will do on unseen data.\n",
    "    for train_index, valid_index in cv_split.split(dates):\n",
    "\n",
    "        # Index dates.\n",
    "        date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "        # Selecting data for y_train and y_valid.\n",
    "        y_train = y.loc[date_train]\n",
    "        y_valid = y.loc[date_valid]\n",
    "\n",
    "        # Selecting data for X_train and X_valid.\n",
    "        X_train = X_C.loc[date_train]\n",
    "        X_valid = X_C.loc[date_valid]\n",
    "\n",
    "        X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        X_train = X_train.set_index([\"date\"])\n",
    "        X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "        y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "        y_train = y_train.set_index([\"date\"])\n",
    "        y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "        # Fitting model.\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Create predictions for Trainning and Validation.\n",
    "        pred = model.predict(X_valid)\n",
    "\n",
    "        # MSE for trainning and validation. \n",
    "        valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "        \n",
    "        if verbose:\n",
    "            # Create predictions for Trainning and Validation.\n",
    "            fit = model.predict(X_train)\n",
    "        \n",
    "            # MSE for trainning and validation. \n",
    "            train_msle.append(float(mean_squared_log_error(y_train[\"sales\"], fit[\"sales\"])))\n",
    "            \n",
    "            a = a+1\n",
    "            print(f\"Fold {a}:\") \n",
    "            print(f\"Training RMSLE: {cp.sqrt(mean_squared_log_error(y_train.sales, fit.sales)):.3f}, Validation RMSLE: {cp.sqrt(mean_squared_log_error(y_valid.sales, pred.sales)):.3f}\")\n",
    "        \n",
    "    if verbose:\n",
    "        # Returns the square root of the average of the MSE.\n",
    "        print(\"Average Across Folds\")\n",
    "        print(f\"Training RMSLE:{np.sqrt(np.mean(train_msle)):.3f}, Validation RMSLE: {np.sqrt(np.mean(valid_msle)):.3f}\")\n",
    "        \n",
    "    return float(np.sqrt(np.mean(valid_msle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07850848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(onpromotion, total_other_promo_store, total_other_city_promo, holiday,\n",
    "                       holiday_description, event, time_since_quake, time_since_quake_sq, state, city, \n",
    "                       dcoilwtico, day, month, workday, payday, onpromotion_lag_1, type, dayofyear, year,\n",
    "                       onpromotion_lag_2, onpromotion_lag_3, onpromotion_lag_4, onpromotion_lag_5,\n",
    "                       onpromotion_lag_6, onpromotion_lag_7, dcoilwtico_lag_1, dcoilwtico_lag_2, \n",
    "                       dcoilwtico_lag_3, dcoilwtico_lag_4, dcoilwtico_lag_5, dcoilwtico_lag_6, \n",
    "                       dcoilwtico_lag_7, sales_one_year_lag, Change_in_oil_prices, promo_last_7_days, X_C=X_C, y=y):\n",
    "    \n",
    "    # Convert non-integer arguments to integers\n",
    "    variable_list = [int(round(Change_in_oil_prices)), int(round(city)),int(round(day)), int(round(dayofyear)), int(round(dcoilwtico)),\n",
    "                        int(round(dcoilwtico_lag_1)), int(round(dcoilwtico_lag_2)), int(round(dcoilwtico_lag_3)), int(round(dcoilwtico_lag_4)),\n",
    "                        int(round(dcoilwtico_lag_5)), int(round(dcoilwtico_lag_6)), int(round(dcoilwtico_lag_7)), int(round(event)),\n",
    "                        int(round(holiday)), int(round(holiday_description)), int(round(month)), int(round(onpromotion)),int(round(onpromotion_lag_1)),\n",
    "                        int(round(onpromotion_lag_2)), int(round(onpromotion_lag_3)), int(round(onpromotion_lag_4)), int(round(onpromotion_lag_5)),\n",
    "                        int(round(onpromotion_lag_6)), int(round(onpromotion_lag_7)), int(round(payday)), int(round(promo_last_7_days)),\n",
    "                        int(round(sales_one_year_lag)), int(round(state)), int(round(time_since_quake)), int(round(time_since_quake_sq)),\n",
    "                        int(round(total_other_city_promo)), int(round(total_other_promo_store)), int(round(type)), int(round(workday)), int(round(year))]\n",
    "    \n",
    "    X_C = X_C.copy()\n",
    "    column_to_remove = []\n",
    "    for i in range(3, X_C.shape[1]):\n",
    "        if variable_list[i-3]==0:\n",
    "            column_to_remove.append(X_C.columns[i])\n",
    "    \n",
    "    X_C.drop(column_to_remove, axis=1, inplace=True)\n",
    "            \n",
    "    xgb_params = {\n",
    "        'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'enable_categorical': True,}\n",
    "\n",
    "    model_2 = XGBRegressor(**xgb_params)\n",
    "\n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "        \n",
    "    list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "    list2 = X_C.columns\n",
    "\n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find the values in set1 that are not in set2\n",
    "    uncommon_values = set1 - set2\n",
    "\n",
    "    # Remove the uncommon values from list1\n",
    "    list1 = [value for value in list1 if value not in uncommon_values]\n",
    "    \n",
    "    numeric_transformer = [\"float\", StandardScaler()]\n",
    "    data_preprocessor = Prepare_data(list1, [numeric_transformer])\n",
    "    \n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=False)\n",
    "    \n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "# Define the parameter space for Bayesian optimization (each feature is a parameter)\n",
    "params = {X_C.columns[i]: (0, 1) for i in range(3, X_C.shape[1])}\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=params,\n",
    "    random_state=1,  # For reproducibility\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log1\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log1\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Perform the optimization\n",
    "optimizer.maximize(init_points=30, n_iter=70)\n",
    "\n",
    "variables = list(optimizer.max[\"params\"].values())\n",
    "variables = [True, True, True] + [x>0.5 for x in variables]\n",
    "X_C = X_C[X_C.columns[variables]]\n",
    "\n",
    "list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "list2 = X_C.columns\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the values in set1 that are not in set2\n",
    "uncommon_values = set1 - set2\n",
    "\n",
    "# Remove the uncommon values from list1\n",
    "list1 = [value for value in list1 if value not in uncommon_values]\n",
    "\n",
    "numeric_transformer = [\"float\", StandardScaler()]\n",
    "data_preprocessor = Prepare_data(list1, [numeric_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ab7d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': -0.7200944000389895, 'params': {'gamma': 1.8341142880593395, 'learning_rate': 0.022189817637013276, 'max_depth': 11.75055013991161, 'n_estimators': 841.6284665485007, 'subsample': 0.6573582590545632}}\n"
     ]
    }
   ],
   "source": [
    "def hyperparameter_optimization(n_estimators, gamma, subsample, max_depth, learning_rate):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    \n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    \n",
    "    params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': max_depth,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': n_estimators,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample}\n",
    "\n",
    "    model_2 = XGBRegressor(**params)\n",
    "\n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "parambounds = {\n",
    "    'learning_rate': (0.00001, 1),\n",
    "    'n_estimators': (0, 1000),\n",
    "    'max_depth': (3,12),\n",
    "    'subsample': (0, 1.0),  \n",
    "    'gamma': (1, 10),\n",
    "    \n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log2\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log2\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=50)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf280501-8dc3-4496-a7c7-a29767594c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training RMSLE: 0.871, Validation RMSLE: 0.754\n",
      "Fold 2:\n",
      "Training RMSLE: 0.806, Validation RMSLE: 0.750\n",
      "Fold 3:\n",
      "Training RMSLE: 0.842, Validation RMSLE: 0.746\n",
      "Fold 4:\n",
      "Training RMSLE: 0.761, Validation RMSLE: 0.622\n",
      "Average Across Folds\n",
      "Training RMSLE:0.821, Validation RMSLE: 0.720\n"
     ]
    }
   ],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "xgb_params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': int(params[\"max_depth\"]),\n",
    "    'learning_rate': params[\"learning_rate\"],\n",
    "    'n_estimators': int(params[\"n_estimators\"]),\n",
    "    'gamma': params[\"gamma\"],\n",
    "    'subsample': params[\"subsample\"]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=True, to_tensor=False)\n",
    "\n",
    "e_1 = 1/Time_Series_CV(model, X_C, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7826d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=True, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "pred_1 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ffd25",
   "metadata": {},
   "source": [
    "## Linear Regression, XGBoost, Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b07b42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = X.copy()\n",
    "\n",
    "X_C = X_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "\n",
    "X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "# X_C = X_C.set_index([\"date\"])\n",
    "# y = y.set_index([\"date\"])\n",
    "\n",
    "X_test_C = X_test.copy()\n",
    "X_test_C = X_test_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_test_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "#.drop([\"state\", \"city\", \"type\", \"dayofyear\", \"year\"], axis=1)\n",
    "X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "894f42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(onpromotion, total_other_promo_store, total_other_city_promo, holiday,\n",
    "                       holiday_description, event, time_since_quake, time_since_quake_sq, state, city, \n",
    "                       dcoilwtico, day, month, workday, payday, onpromotion_lag_1, type, dayofyear, year,\n",
    "                       onpromotion_lag_2, onpromotion_lag_3, onpromotion_lag_4, onpromotion_lag_5,\n",
    "                       onpromotion_lag_6, onpromotion_lag_7, dcoilwtico_lag_1, dcoilwtico_lag_2, \n",
    "                       dcoilwtico_lag_3, dcoilwtico_lag_4, dcoilwtico_lag_5, dcoilwtico_lag_6, \n",
    "                       dcoilwtico_lag_7, sales_one_year_lag, Change_in_oil_prices, promo_last_7_days, X_C=X_C, y=y):\n",
    "    \n",
    "    # Convert non-integer arguments to integers\n",
    "    # Convert non-integer arguments to integers\n",
    "    variable_list = [int(round(Change_in_oil_prices)), int(round(city)),int(round(day)), int(round(dayofyear)), int(round(dcoilwtico)),\n",
    "                        int(round(dcoilwtico_lag_1)), int(round(dcoilwtico_lag_2)), int(round(dcoilwtico_lag_3)), int(round(dcoilwtico_lag_4)),\n",
    "                        int(round(dcoilwtico_lag_5)), int(round(dcoilwtico_lag_6)), int(round(dcoilwtico_lag_7)), int(round(event)),\n",
    "                        int(round(holiday)), int(round(holiday_description)), int(round(month)), int(round(onpromotion)),int(round(onpromotion_lag_1)),\n",
    "                        int(round(onpromotion_lag_2)), int(round(onpromotion_lag_3)), int(round(onpromotion_lag_4)), int(round(onpromotion_lag_5)),\n",
    "                        int(round(onpromotion_lag_6)), int(round(onpromotion_lag_7)), int(round(payday)), int(round(promo_last_7_days)),\n",
    "                        int(round(sales_one_year_lag)), int(round(state)), int(round(time_since_quake)), int(round(time_since_quake_sq)),\n",
    "                        int(round(total_other_city_promo)), int(round(total_other_promo_store)), int(round(type)), int(round(workday)), int(round(year))]\n",
    "    X_C = X_C.copy()\n",
    "    column_to_remove = []\n",
    "    for i in range(3, X_C.shape[1]):\n",
    "        if variable_list[i-3]==0:\n",
    "            column_to_remove.append(X_C.columns[i])\n",
    "    \n",
    "    X_C.drop(column_to_remove, axis=1, inplace=True)\n",
    "            \n",
    "    xgb_params = {\n",
    "        'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'enable_categorical': True,}\n",
    "\n",
    "    model_2 = XGBRegressor(**xgb_params)\n",
    "\n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "    list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "    list2 = X_C.columns\n",
    "\n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find the values in set1 that are not in set2\n",
    "    uncommon_values = set1 - set2\n",
    "\n",
    "    # Remove the uncommon values from list1\n",
    "    list1 = [value for value in list1 if value not in uncommon_values]\n",
    "    \n",
    "    numeric_transformer = [\"float\", StandardScaler()]\n",
    "    data_preprocessor = Prepare_data(list1, [numeric_transformer])\n",
    "    \n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=False, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "# Define the parameter space for Bayesian optimization (each feature is a parameter)\n",
    "params = {X_C.columns[i]: (False, True) for i in range(3, X_C.shape[1])}\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=params,\n",
    "    random_state=1,  # For reproducibility\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log3\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log3\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Perform the optimization\n",
    "optimizer.maximize(init_points=30, n_iter=70)\n",
    "\n",
    "variables = list(optimizer.max[\"params\"].values())\n",
    "variables = [True, True, True] + [x>0.5 for x in variables]\n",
    "X_C = X_C[X_C.columns[variables]]\n",
    "\n",
    "list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "list2 = X_C.columns\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the values in set1 that are not in set2\n",
    "uncommon_values = set1 - set2\n",
    "\n",
    "# Remove the uncommon values from list1\n",
    "list1 = [value for value in list1 if value not in uncommon_values]\n",
    "\n",
    "numeric_transformer = [\"float\", StandardScaler()]\n",
    "data_preprocessor = Prepare_data(list1, [numeric_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c511603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': -0.6686000891811915, 'params': {'gamma': 3.817714029576126, 'learning_rate': 0.3625283226197647, 'max_depth': 11.818925416760282, 'n_estimators': 7.132832399346034, 'subsample': 0.8966917484334722}}\n"
     ]
    }
   ],
   "source": [
    "def hyperparameter_optimization(n_estimators, gamma, subsample, max_depth, learning_rate):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    \n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    \n",
    "    params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': max_depth,\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': n_estimators,\n",
    "    'gamma': gamma,\n",
    "    'subsample': subsample}\n",
    "\n",
    "    model_2 = XGBRegressor(**params)\n",
    "  \n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=False, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "parambounds = {\n",
    "    'learning_rate': (0.00001, 1),\n",
    "    'n_estimators': (0, 500),\n",
    "    'max_depth': (3,12),\n",
    "    'subsample': (0.0001, 1.0),  \n",
    "    'gamma': (3, 8),\n",
    "    \n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log4\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log4\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=50,)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11d5a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training RMSLE: 0.612, Validation RMSLE: 0.694\n",
      "Fold 2:\n",
      "Training RMSLE: 0.642, Validation RMSLE: 0.711\n",
      "Fold 3:\n",
      "Training RMSLE: 0.634, Validation RMSLE: 0.686\n",
      "Fold 4:\n",
      "Training RMSLE: 0.628, Validation RMSLE: 0.575\n",
      "Average Across Folds\n",
      "Training RMSLE:0.629, Validation RMSLE: 0.669\n"
     ]
    }
   ],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "xgb_params = {\n",
    "    'tree_method': 'gpu_hist',  # Specify GPU usage\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': int(params[\"max_depth\"]),\n",
    "    'learning_rate': params[\"learning_rate\"],\n",
    "    'n_estimators': int(params[\"n_estimators\"]),\n",
    "    'gamma': params[\"gamma\"],\n",
    "    'subsample': params[\"subsample\"]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=False, to_tensor=False)\n",
    "\n",
    "e_2 = 1/Time_Series_CV(model, X_C, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96937b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=False, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "pred_2 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a8cd0",
   "metadata": {},
   "source": [
    "## Linear Regression, Random Forest Regressor, Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee5f3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = X.copy()\n",
    "\n",
    "X_C[\"family\"] = X_C[\"family\"].cat.codes\n",
    "X_C[\"store_nbr\"] = X_C[\"store_nbr\"].cat.codes\n",
    "X_C[\"holiday\"] = X_C[\"holiday\"].cat.codes\n",
    "X_C[\"event\"] = X[\"event\"].cat.codes\n",
    "X_C[\"city\"] = X_C[\"city\"].cat.codes\n",
    "X_C[\"state\"] = X_C[\"state\"].cat.codes\n",
    "X_C[\"type\"] = X_C[\"type\"].cat.codes\n",
    "X_C[\"payday\"] = X_C[\"payday\"].cat.codes\n",
    "X_C[\"workday\"] = X_C[\"workday\"].cat.codes\n",
    "X_C[\"holiday_description\"] = X_C[\"holiday_description\"].cat.codes\n",
    "\n",
    "X_C = X_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "\n",
    "X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "\n",
    "X_test_C = X_test.copy()\n",
    "X_test_C = X_test_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_test_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e4d042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def objective_function(onpromotion, total_other_promo_store, total_other_city_promo, holiday,\n",
    "                       holiday_description, event, time_since_quake, time_since_quake_sq, state, city, \n",
    "                       dcoilwtico, day, month, workday, payday, onpromotion_lag_1, type, dayofyear, year,\n",
    "                       onpromotion_lag_2, onpromotion_lag_3, onpromotion_lag_4, onpromotion_lag_5,\n",
    "                       onpromotion_lag_6, onpromotion_lag_7, dcoilwtico_lag_1, dcoilwtico_lag_2, \n",
    "                       dcoilwtico_lag_3, dcoilwtico_lag_4, dcoilwtico_lag_5, dcoilwtico_lag_6, \n",
    "                       dcoilwtico_lag_7, sales_one_year_lag, Change_in_oil_prices, promo_last_7_days, X_C=X_C, y=y):\n",
    "    \n",
    "    # Convert non-integer arguments to integers\n",
    "    variable_list = [int(round(Change_in_oil_prices)), int(round(city)),int(round(day)), int(round(dayofyear)), int(round(dcoilwtico)),\n",
    "                        int(round(dcoilwtico_lag_1)), int(round(dcoilwtico_lag_2)), int(round(dcoilwtico_lag_3)), int(round(dcoilwtico_lag_4)),\n",
    "                        int(round(dcoilwtico_lag_5)), int(round(dcoilwtico_lag_6)), int(round(dcoilwtico_lag_7)), int(round(event)),\n",
    "                        int(round(holiday)), int(round(holiday_description)), int(round(month)), int(round(onpromotion)),int(round(onpromotion_lag_1)),\n",
    "                        int(round(onpromotion_lag_2)), int(round(onpromotion_lag_3)), int(round(onpromotion_lag_4)), int(round(onpromotion_lag_5)),\n",
    "                        int(round(onpromotion_lag_6)), int(round(onpromotion_lag_7)), int(round(payday)), int(round(promo_last_7_days)),\n",
    "                        int(round(sales_one_year_lag)), int(round(state)), int(round(time_since_quake)), int(round(time_since_quake_sq)),\n",
    "                        int(round(total_other_city_promo)), int(round(total_other_promo_store)), int(round(type)), int(round(workday)), int(round(year))]\n",
    "    \n",
    "    X_C = X_C.copy()\n",
    "    column_to_remove = []\n",
    "    for i in range(3, X_C.shape[1]):\n",
    "        if variable_list[i-3]==0:\n",
    "            column_to_remove.append(X_C.columns[i])\n",
    "    \n",
    "    X_C.drop(column_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    model_2 = RandomForestRegressor()\n",
    "\n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "    list2 = X_C.columns\n",
    "\n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find the values in set1 that are not in set2\n",
    "    uncommon_values = set1 - set2\n",
    "\n",
    "    # Remove the uncommon values from list1\n",
    "    list1 = [value for value in list1 if value not in uncommon_values]\n",
    "    \n",
    "    numeric_transformer = [\"float\", StandardScaler()]\n",
    "    categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "    data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])\n",
    "\n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "# Define the parameter space for Bayesian optimization (each feature is a parameter)\n",
    "params = {X_C.columns[i]: (False, True) for i in range(3, X_C.shape[1])}\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=params,\n",
    "    random_state=1,  # For reproducibility\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log5\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log5\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Perform the optimization\n",
    "optimizer.maximize(init_points=30, n_iter=70)\n",
    "\n",
    "variables = list(optimizer.max[\"params\"].values())\n",
    "variables = [True, True, True] + [x>0.5 for x in variables]\n",
    "X_C = X_C[X_C.columns[variables]]\n",
    "\n",
    "list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "list2 = X_C.columns\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the values in set1 that are not in set2\n",
    "uncommon_values = set1 - set2\n",
    "\n",
    "# Remove the uncommon values from list1\n",
    "list1 = [value for value in list1 if value not in uncommon_values]\n",
    "\n",
    "numeric_transformer = [\"float\", StandardScaler()]\n",
    "categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap, max_samples, criterion):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    \n",
    "    if max_features>1:\n",
    "        if max_features>2:\n",
    "            max_features = 'sqrt'\n",
    "        else:\n",
    "            max_features = 'auto'\n",
    "    else:\n",
    "        max_features = 'log2'\n",
    "\n",
    "    if criterion>1:\n",
    "        criterion = 'mse'\n",
    "    else:\n",
    "        criterion = 'mae'\n",
    "    \n",
    "    params = {\n",
    "    'n_estimators': int(round(n_estimators)),\n",
    "    'max_depth': int(round(max_depth)),\n",
    "    'min_samples_split': int(round(min_samples_split)),\n",
    "    'min_samples_leaf': int(round(min_samples_leaf)),\n",
    "    'max_features': int(round(max_features)),\n",
    "    'bootstrap': int(round(bootstrap)),\n",
    "    'max_samples': max_samples,\n",
    "    'criterion': criterion}\n",
    "\n",
    "    model_2 = RandomForestRegressor(**params)\n",
    "    \n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y, verbose=True)\n",
    "\n",
    "param_bounds = {\n",
    "    'n_estimators': (100, 1000),  # Number of trees in the forest\n",
    "    'max_depth': (3, 20),  # Maximum depth of the trees\n",
    "    'min_samples_split': (2, 20),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': (1, 20),  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': (0, 3),  # Number of features to consider when looking for the best split\n",
    "    'bootstrap': (True, False),  # Whether bootstrap samples are used when building trees\n",
    "    'max_samples': (0.1, 1.0),  # Number of samples to draw from X to train each base estimator\n",
    "    'criterion': (0, 1),  # The function used to measure the quality of a split\n",
    "}\n",
    "    \n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log6\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log6\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=50,)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd623c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "if params[4]>1:\n",
    "    if params[4]>2:\n",
    "        max_features = 'sqrt'\n",
    "    else:\n",
    "        max_features = 'auto'\n",
    "else:\n",
    "    max_features = 'log2'\n",
    "    \n",
    "if params[7]>1:\n",
    "    criterion = 'mse'\n",
    "else:\n",
    "    criterion = 'mae'\n",
    "    \n",
    "rfr_params = {\n",
    "    'n_estimators': int(round(params[0])),  # Number of trees in the forest\n",
    "    'max_depth': int(round(params[1])),  # Maximum depth of the trees\n",
    "    'min_samples_split': int(round(params[2])),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': int(round(params[3])),  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': max_features,  # Number of features to consider when looking for the best split\n",
    "    'bootstrap': int(round(params[5])),  # Whether bootstrap samples are used when building trees\n",
    "    'max_samples': params[6],  # Number of samples to draw from X to train each base estimator\n",
    "    'criterion': criterion,  # The function used to measure the quality of a split\n",
    "}\n",
    "\n",
    "rfr = RandomForestRegressor(**rfr_params)\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, rfr, Boosted=True, to_tensor=False)\n",
    "\n",
    "e_3 = 1/Time_Series_CV(model, X_C, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623323f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, rfr, Boosted=True, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "pred_3 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d5d80",
   "metadata": {},
   "source": [
    "## Linear Regression, Random Forest Regressor, Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = X.copy()\n",
    "\n",
    "X_C[\"family\"] = X_C[\"family\"].cat.codes\n",
    "X_C[\"store_nbr\"] = X_C[\"store_nbr\"].cat.codes\n",
    "X_C[\"holiday\"] = X_C[\"holiday\"].cat.codes\n",
    "X_C[\"event\"] = X[\"event\"].cat.codes\n",
    "X_C[\"city\"] = X_C[\"city\"].cat.codes\n",
    "X_C[\"state\"] = X_C[\"state\"].cat.codes\n",
    "X_C[\"type\"] = X_C[\"type\"].cat.codes\n",
    "X_C[\"payday\"] = X_C[\"payday\"].cat.codes\n",
    "X_C[\"workday\"] = X_C[\"workday\"].cat.codes\n",
    "X_C[\"holiday_description\"] = X_C[\"holiday_description\"].cat.codes\n",
    "\n",
    "X_C = X_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "\n",
    "X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "\n",
    "X_test_C = X_test.copy()\n",
    "X_test_C = X_test_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_test_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ee15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(onpromotion, total_other_promo_store, total_other_city_promo, holiday,\n",
    "                       holiday_description, event, time_since_quake, time_since_quake_sq, state, city, \n",
    "                       dcoilwtico, day, month, workday, payday, onpromotion_lag_1, type, dayofyear, year,\n",
    "                       onpromotion_lag_2, onpromotion_lag_3, onpromotion_lag_4, onpromotion_lag_5,\n",
    "                       onpromotion_lag_6, onpromotion_lag_7, dcoilwtico_lag_1, dcoilwtico_lag_2, \n",
    "                       dcoilwtico_lag_3, dcoilwtico_lag_4, dcoilwtico_lag_5, dcoilwtico_lag_6, \n",
    "                       dcoilwtico_lag_7, sales_one_year_lag, Change_in_oil_prices, promo_last_7_days, X_C=X_C, y=y):\n",
    "    \n",
    "    # Convert non-integer arguments to integers\n",
    "    variable_list = [int(round(Change_in_oil_prices)), int(round(city)),int(round(day)), int(round(dayofyear)), int(round(dcoilwtico)),\n",
    "                        int(round(dcoilwtico_lag_1)), int(round(dcoilwtico_lag_2)), int(round(dcoilwtico_lag_3)), int(round(dcoilwtico_lag_4)),\n",
    "                        int(round(dcoilwtico_lag_5)), int(round(dcoilwtico_lag_6)), int(round(dcoilwtico_lag_7)), int(round(event)),\n",
    "                        int(round(holiday)), int(round(holiday_description)), int(round(month)), int(round(onpromotion)),int(round(onpromotion_lag_1)),\n",
    "                        int(round(onpromotion_lag_2)), int(round(onpromotion_lag_3)), int(round(onpromotion_lag_4)), int(round(onpromotion_lag_5)),\n",
    "                        int(round(onpromotion_lag_6)), int(round(onpromotion_lag_7)), int(round(payday)), int(round(promo_last_7_days)),\n",
    "                        int(round(sales_one_year_lag)), int(round(state)), int(round(time_since_quake)), int(round(time_since_quake_sq)),\n",
    "                        int(round(total_other_city_promo)), int(round(total_other_promo_store)), int(round(type)), int(round(workday)), int(round(year))]\n",
    "    \n",
    "    X_C = X_C.copy()\n",
    "    column_to_remove = []\n",
    "    for i in range(3, X_C.shape[1]):\n",
    "        if variable_list[i-3]==0:\n",
    "            column_to_remove.append(X_C.columns[i])\n",
    "    \n",
    "    X_C.drop(column_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    model_2 = RandomForestRegressor()\n",
    "\n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "    list2 = X_C.columns\n",
    "\n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find the values in set1 that are not in set2\n",
    "    uncommon_values = set1 - set2\n",
    "\n",
    "    # Remove the uncommon values from list1\n",
    "    list1 = [value for value in list1 if value not in uncommon_values]\n",
    "    \n",
    "    numeric_transformer = [\"float\", StandardScaler()]\n",
    "    categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "    data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])\n",
    "\n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=False, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "# Define the parameter space for Bayesian optimization (each feature is a parameter)\n",
    "params = {X_C.columns[i]: (0, 1) for i in range(3, X_C.shape[1])}\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=params,\n",
    "    random_state=1,  # For reproducibility\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log7\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log7\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Perform the optimization\n",
    "optimizer.maximize(init_points=30, n_iter=70)\n",
    "\n",
    "variables = list(optimizer.max[\"params\"].values())\n",
    "variables = [True, True, True] + [x>0.5 for x in variables]\n",
    "X_C = X_C[X_C.columns[variables]]\n",
    "\n",
    "list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "list2 = X_C.columns\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the values in set1 that are not in set2\n",
    "uncommon_values = set1 - set2\n",
    "\n",
    "# Remove the uncommon values from list1\n",
    "list1 = [value for value in list1 if value not in uncommon_values]\n",
    "\n",
    "numeric_transformer = [\"float\", StandardScaler()]\n",
    "categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, bootstrap, max_samples, criterion):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    \n",
    "    if max_features>1:\n",
    "        if max_features>2:\n",
    "            max_features = 'sqrt'\n",
    "        else:\n",
    "            max_features = 'auto'\n",
    "    else:\n",
    "        max_features = 'log2'\n",
    "\n",
    "    if criterion>.5:\n",
    "        criterion = 'mse'\n",
    "    else:\n",
    "        criterion = 'mae'\n",
    "    \n",
    "    params = {\n",
    "    'n_estimators': int(round(n_estimators)),\n",
    "    'max_depth': int(round(max_depth)),\n",
    "    'min_samples_split': int(round(min_samples_split)),\n",
    "    'min_samples_leaf': int(round(min_samples_leaf)),\n",
    "    'max_features': max_features,\n",
    "    'bootstrap': int(round(bootstrap)),\n",
    "    'max_samples': max_samples,\n",
    "    'criterion': criterion}\n",
    "\n",
    "    model_2 = RandomForestRegressor(**params)\n",
    "\n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=False, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "param_bounds = {\n",
    "    'n_estimators': (100, 1000),  # Number of trees in the forest\n",
    "    'max_depth': (3, 20),  # Maximum depth of the trees\n",
    "    'min_samples_split': (2, 20),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': (1, 20),  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': (0, 3),  # Number of features to consider when looking for the best split\n",
    "    'bootstrap': (True, False),  # Whether bootstrap samples are used when building trees\n",
    "    'max_samples': (0.1, 1.0),  # Number of samples to draw from X to train each base estimator\n",
    "    'criterion': (0, 1),  # The function used to measure the quality of a split\n",
    "}\n",
    "    \n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log8\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log8\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=50,)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caaa0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "if params[4]>1:\n",
    "    if params[4]>2:\n",
    "        max_features = 'sqrt'\n",
    "    else:\n",
    "        max_features = 'auto'\n",
    "else:\n",
    "    max_features = 'log2'\n",
    "    \n",
    "if params[7]>.5:\n",
    "    criterion = 'mse'\n",
    "else:\n",
    "    criterion = 'mae'\n",
    "    \n",
    "rfr_params = {\n",
    "    'n_estimators': int(round(params[0])),  # Number of trees in the forest\n",
    "    'max_depth': int(round(params[1])),  # Maximum depth of the trees\n",
    "    'min_samples_split': int(round(params[2])),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': int(round(params[3])),  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': max_features,  # Number of features to consider when looking for the best split\n",
    "    'bootstrap': int(round(params[5])),  # Whether bootstrap samples are used when building trees\n",
    "    'max_samples': params[6],  # Number of samples to draw from X to train each base estimator\n",
    "    'criterion': criterion,  # The function used to measure the quality of a split\n",
    "}\n",
    "\n",
    "rfr = RandomForestRegressor(**rfr_params)\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, rfr, Boosted=False, to_tensor=False)\n",
    "\n",
    "e_4 = 1/Time_Series_CV(model, X_C, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, rfr, Boosted=False, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "pred_4 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be0b25",
   "metadata": {},
   "source": [
    "## Linear Regression, K-NN Regressor, Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bde440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = X.copy()\n",
    "\n",
    "X_C[\"family\"] = X_C[\"family\"].cat.codes\n",
    "X_C[\"store_nbr\"] = X_C[\"store_nbr\"].cat.codes\n",
    "X_C[\"holiday\"] = X_C[\"holiday\"].cat.codes\n",
    "X_C[\"event\"] = X[\"event\"].cat.codes\n",
    "X_C[\"city\"] = X_C[\"city\"].cat.codes\n",
    "X_C[\"state\"] = X_C[\"state\"].cat.codes\n",
    "X_C[\"type\"] = X_C[\"type\"].cat.codes\n",
    "X_C[\"payday\"] = X_C[\"payday\"].cat.codes\n",
    "X_C[\"workday\"] = X_C[\"workday\"].cat.codes\n",
    "X_C[\"holiday_description\"] = X_C[\"holiday_description\"].cat.codes\n",
    "\n",
    "X_C = X_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "\n",
    "X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "\n",
    "X_test_C = X_test.copy()\n",
    "X_test_C = X_test_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_test_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225cfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(onpromotion, total_other_promo_store, total_other_city_promo, holiday,\n",
    "                       holiday_description, event, time_since_quake, time_since_quake_sq, state, city, \n",
    "                       dcoilwtico, day, month, workday, payday, onpromotion_lag_1, type, dayofyear, year,\n",
    "                       onpromotion_lag_2, onpromotion_lag_3, onpromotion_lag_4, onpromotion_lag_5,\n",
    "                       onpromotion_lag_6, onpromotion_lag_7, dcoilwtico_lag_1, dcoilwtico_lag_2, \n",
    "                       dcoilwtico_lag_3, dcoilwtico_lag_4, dcoilwtico_lag_5, dcoilwtico_lag_6, \n",
    "                       dcoilwtico_lag_7, sales_one_year_lag, Change_in_oil_prices, promo_last_7_days, X_C=X_C, y=y):\n",
    "    \n",
    "    # Convert non-integer arguments to integers\n",
    "    variable_list = [int(round(Change_in_oil_prices)), int(round(city)),int(round(day)), int(round(dayofyear)), int(round(dcoilwtico)),\n",
    "                        int(round(dcoilwtico_lag_1)), int(round(dcoilwtico_lag_2)), int(round(dcoilwtico_lag_3)), int(round(dcoilwtico_lag_4)),\n",
    "                        int(round(dcoilwtico_lag_5)), int(round(dcoilwtico_lag_6)), int(round(dcoilwtico_lag_7)), int(round(event)),\n",
    "                        int(round(holiday)), int(round(holiday_description)), int(round(month)), int(round(onpromotion)),int(round(onpromotion_lag_1)),\n",
    "                        int(round(onpromotion_lag_2)), int(round(onpromotion_lag_3)), int(round(onpromotion_lag_4)), int(round(onpromotion_lag_5)),\n",
    "                        int(round(onpromotion_lag_6)), int(round(onpromotion_lag_7)), int(round(payday)), int(round(promo_last_7_days)),\n",
    "                        int(round(sales_one_year_lag)), int(round(state)), int(round(time_since_quake)), int(round(time_since_quake_sq)),\n",
    "                        int(round(total_other_city_promo)), int(round(total_other_promo_store)), int(round(type)), int(round(workday)), int(round(year))]\n",
    "    \n",
    "    X_C = X_C.copy()\n",
    "    column_to_remove = []\n",
    "    for i in range(3, X_C.shape[1]):\n",
    "        if variable_list[i-3]==0:\n",
    "            column_to_remove.append(X_C.columns[i])\n",
    "    \n",
    "    X_C.drop(column_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    model_2 = KNeighborsRegressor()\n",
    "\n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "    list2 = X_C.columns\n",
    "\n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find the values in set1 that are not in set2\n",
    "    uncommon_values = set1 - set2\n",
    "\n",
    "    # Remove the uncommon values from list1\n",
    "    list1 = [value for value in list1 if value not in uncommon_values]\n",
    "    \n",
    "    numeric_transformer = [\"float\", StandardScaler()]\n",
    "    categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "    data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    \n",
    "    \n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "# Define the parameter space for Bayesian optimization (each feature is a parameter)\n",
    "params = {X_C.columns[i]: (0, 1) for i in range(3, X_C.shape[1])}\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=params,\n",
    "    random_state=1,  # For reproducibility\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log9\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log9\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Perform the optimization\n",
    "optimizer.maximize(init_points=30, n_iter=70)\n",
    "\n",
    "variables = list(optimizer.max[\"params\"].values())\n",
    "variables = [True, True, True] + [x>0.5 for x in variables]\n",
    "X_C = X_C[X_C.columns[variables]]\n",
    "\n",
    "list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "list2 = X_C.columns\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the values in set1 that are not in set2\n",
    "uncommon_values = set1 - set2\n",
    "\n",
    "# Remove the uncommon values from list1\n",
    "list1 = [value for value in list1 if value not in uncommon_values]\n",
    "\n",
    "numeric_transformer = [\"float\", StandardScaler()]\n",
    "categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(n_neighbors, weights, algorithm, leaf_size, metric):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    if metric>1:\n",
    "        if metric>2:\n",
    "            metric = 'euclidean'\n",
    "        else:\n",
    "            metric = 'manhattan'\n",
    "    else:\n",
    "        metric = 'minkowski'\n",
    "\n",
    "\n",
    "    if algorithm>1:\n",
    "        if algorithm>2:\n",
    "            algorithm = 'auto'\n",
    "        else:\n",
    "            if algorithm>3:\n",
    "                algorithm = 'ball_tree'\n",
    "            else:\n",
    "                algorithm = 'kd_tree'\n",
    "    else:\n",
    "        algorithm = 'brute'\n",
    "\n",
    "\n",
    "    if weights>.5:\n",
    "        weights = 'uniform'\n",
    "    else:\n",
    "        weights = 'distance'\n",
    "    \n",
    "    param = {\n",
    "    'n_neighbors': int(round(n_neighbors)),\n",
    "    'weights': weights,\n",
    "    'algorithm': algorithm,\n",
    "    'leaf_size': int(round(leaf_size)),\n",
    "    'metric': metric}\n",
    "\n",
    "    model_2 = KNeighborsRegressor(**params)\n",
    "\n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "param_bounds = {\n",
    "    'n_neighbors': (1, 20),\n",
    "    'weights': (0, 1),\n",
    "    'algorithm': (0, 4),\n",
    "    'leaf_size': (10, 50),\n",
    "    'metric': (0, 3),\n",
    "}\n",
    "    \n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log10\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log10\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=50,)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "if params[4]>1:\n",
    "    if params[4]>2:\n",
    "        metric = 'euclidean'\n",
    "    else:\n",
    "        metric = 'manhattan'\n",
    "else:\n",
    "    metric = 'minkowski'\n",
    "\n",
    "\n",
    "if params[2]>1:\n",
    "    if params[2]>2:\n",
    "        algorithm = 'auto'\n",
    "    else:\n",
    "        if params[2]>3:\n",
    "            algorithm = 'ball_tree'\n",
    "        else:\n",
    "            algorithm = 'kd_tree'\n",
    "else:\n",
    "    algorithm = 'brute'\n",
    "\n",
    "\n",
    "if params[1]>.5:\n",
    "    weights = 'uniform'\n",
    "else:\n",
    "    weights = 'distance'\n",
    "\n",
    "param = {'n_neighbors': int(round(params[0])),\n",
    "        'weights': weights,\n",
    "        'algorithm': algorithm,\n",
    "        'leaf_size': int(round(params[3])),\n",
    "        'metric': metric}\n",
    "\n",
    "knn = KNeighborsRegressor(**knn_params)\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, knn, Boosted=True, to_tensor=False)\n",
    " \n",
    "e_5 = 1/Time_Series_CV(model, X_C, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, knn, Boosted=True, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "pred_5 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d132bc",
   "metadata": {},
   "source": [
    "## Linear Regression, K-NN Regressor, Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = X.copy()\n",
    "\n",
    "X_C[\"family\"] = X_C[\"family\"].cat.codes\n",
    "X_C[\"store_nbr\"] = X_C[\"store_nbr\"].cat.codes\n",
    "X_C[\"holiday\"] = X_C[\"holiday\"].cat.codes\n",
    "X_C[\"event\"] = X[\"event\"].cat.codes\n",
    "X_C[\"city\"] = X_C[\"city\"].cat.codes\n",
    "X_C[\"state\"] = X_C[\"state\"].cat.codes\n",
    "X_C[\"type\"] = X_C[\"type\"].cat.codes\n",
    "X_C[\"payday\"] = X_C[\"payday\"].cat.codes\n",
    "X_C[\"workday\"] = X_C[\"workday\"].cat.codes\n",
    "X_C[\"holiday_description\"] = X_C[\"holiday_description\"].cat.codes\n",
    "\n",
    "X_C = X_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "\n",
    "X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "\n",
    "X_test_C = X_test.copy()\n",
    "X_test_C = X_test_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_test_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(onpromotion, total_other_promo_store, total_other_city_promo, holiday,\n",
    "                       holiday_description, event, time_since_quake, time_since_quake_sq, state, city, \n",
    "                       dcoilwtico, day, month, workday, payday, onpromotion_lag_1, type, dayofyear, year,\n",
    "                       onpromotion_lag_2, onpromotion_lag_3, onpromotion_lag_4, onpromotion_lag_5,\n",
    "                       onpromotion_lag_6, onpromotion_lag_7, dcoilwtico_lag_1, dcoilwtico_lag_2, \n",
    "                       dcoilwtico_lag_3, dcoilwtico_lag_4, dcoilwtico_lag_5, dcoilwtico_lag_6, \n",
    "                       dcoilwtico_lag_7, sales_one_year_lag, Change_in_oil_prices, promo_last_7_days, X_C=X_C, y=y):\n",
    "    \n",
    "    # Convert non-integer arguments to integers\n",
    "    variable_list = [int(round(Change_in_oil_prices)), int(round(city)),int(round(day)), int(round(dayofyear)), int(round(dcoilwtico)),\n",
    "                        int(round(dcoilwtico_lag_1)), int(round(dcoilwtico_lag_2)), int(round(dcoilwtico_lag_3)), int(round(dcoilwtico_lag_4)),\n",
    "                        int(round(dcoilwtico_lag_5)), int(round(dcoilwtico_lag_6)), int(round(dcoilwtico_lag_7)), int(round(event)),\n",
    "                        int(round(holiday)), int(round(holiday_description)), int(round(month)), int(round(onpromotion)),int(round(onpromotion_lag_1)),\n",
    "                        int(round(onpromotion_lag_2)), int(round(onpromotion_lag_3)), int(round(onpromotion_lag_4)), int(round(onpromotion_lag_5)),\n",
    "                        int(round(onpromotion_lag_6)), int(round(onpromotion_lag_7)), int(round(payday)), int(round(promo_last_7_days)),\n",
    "                        int(round(sales_one_year_lag)), int(round(state)), int(round(time_since_quake)), int(round(time_since_quake_sq)),\n",
    "                        int(round(total_other_city_promo)), int(round(total_other_promo_store)), int(round(type)), int(round(workday)), int(round(year))]\n",
    "    \n",
    "    X_C = X_C.copy()\n",
    "    column_to_remove = []\n",
    "    for i in range(3, X_C.shape[1]):\n",
    "        if variable_list[i-3]==0:\n",
    "            column_to_remove.append(X_C.columns[i])\n",
    "    \n",
    "    X_C.drop(column_to_remove, axis=1, inplace=True)\n",
    "\n",
    "    model_2 = KNeighborsRegressor()\n",
    "\n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "    list2 = X_C.columns\n",
    "\n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find the values in set1 that are not in set2\n",
    "    uncommon_values = set1 - set2\n",
    "\n",
    "    # Remove the uncommon values from list1\n",
    "    list1 = [value for value in list1 if value not in uncommon_values]\n",
    "    \n",
    "    numeric_transformer = [\"float\", StandardScaler()]\n",
    "    categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "    data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    \n",
    "\n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=False, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "# Define the parameter space for Bayesian optimization (each feature is a parameter)\n",
    "params = {X_C.columns[i]: (0, 1) for i in range(3, X_C.shape[1])}\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,\n",
    "    pbounds=params,\n",
    "    random_state=1,  # For reproducibility\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log11\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log11\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Perform the optimization\n",
    "optimizer.maximize(init_points=30, n_iter=70)\n",
    "\n",
    "variables = list(optimizer.max[\"params\"].values())\n",
    "variables = [True, True, True] + [x>0.5 for x in variables]\n",
    "X_C = X_C[X_C.columns[variables]]\n",
    "\n",
    "list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "list2 = X_C.columns\n",
    "\n",
    "# Convert lists to sets\n",
    "set1 = set(list1)\n",
    "set2 = set(list2)\n",
    "\n",
    "# Find the values in set1 that are not in set2\n",
    "uncommon_values = set1 - set2\n",
    "\n",
    "# Remove the uncommon values from list1\n",
    "list1 = [value for value in list1 if value not in uncommon_values]\n",
    "\n",
    "numeric_transformer = [\"float\", StandardScaler()]\n",
    "categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88724cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(n_neighbors, weights, algorithm, leaf_size, metric):\n",
    "    \n",
    "    \n",
    "    model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    if metric>1:\n",
    "        if metric>2:\n",
    "            metric = 'euclidean'\n",
    "        else:\n",
    "            metric = 'manhattan'\n",
    "    else:\n",
    "        metric = 'minkowski'\n",
    "\n",
    "\n",
    "    if algorithm>1:\n",
    "        if algorithm>2:\n",
    "            algorithm = 'auto'\n",
    "        else:\n",
    "            if algorithm>3:\n",
    "                algorithm = 'ball_tree'\n",
    "            else:\n",
    "                algorithm = 'kd_tree'\n",
    "    else:\n",
    "        algorithm = 'brute'\n",
    "\n",
    "\n",
    "    if weights>.5:\n",
    "        weights = 'uniform'\n",
    "    else:\n",
    "        weights = 'distance'\n",
    "    \n",
    "    param = {\n",
    "    'n_neighbors': int(round(n_neighbors)),\n",
    "    'weights': weights,\n",
    "    'algorithm': algorithm,\n",
    "    'leaf_size': int(round(leaf_size)),\n",
    "    'metric': metric}\n",
    "\n",
    "    model_2 = KNeighborsRegressor(**params)\n",
    "\n",
    "    model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=False, to_tensor=False)\n",
    "\n",
    "    return -Time_Series_CV(model, X_C, y)\n",
    "\n",
    "param_bounds = {\n",
    "    'n_neighbors': (1, 20),\n",
    "    'weights': (0, 1),\n",
    "    'algorithm': (0, 4),\n",
    "    'leaf_size': (10, 50),\n",
    "    'metric': (0, 3),\n",
    "}\n",
    "    \n",
    "optimizer = BayesianOptimization(\n",
    "    f=hyperparameter_optimization,\n",
    "    pbounds=parambounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "logger = JSONLogger(path=\"./Logs/logs.log12\")\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "try:\n",
    "    load_logs(optimizer, logs[\"./Logs/logs.log12\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "optimizer.maximize(init_points=20, n_iter=50,)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = optimizer.max[\"params\"]\n",
    "\n",
    "if params[4]>1:\n",
    "    if params[4]>2:\n",
    "        metric = 'euclidean'\n",
    "    else:\n",
    "        metric = 'manhattan'\n",
    "else:\n",
    "    metric = 'minkowski'\n",
    "\n",
    "\n",
    "if params[2]>1:\n",
    "    if params[2]>2:\n",
    "        algorithm = 'auto'\n",
    "    else:\n",
    "        if params[2]>3:\n",
    "            algorithm = 'ball_tree'\n",
    "        else:\n",
    "            algorithm = 'kd_tree'\n",
    "else:\n",
    "    algorithm = 'brute'\n",
    "\n",
    "\n",
    "if params[1]>.5:\n",
    "    weights = 'uniform'\n",
    "else:\n",
    "    weights = 'distance'\n",
    "\n",
    "param = {'n_neighbors': int(round(params[0])),\n",
    "        'weights': weights,\n",
    "        'algorithm': algorithm,\n",
    "        'leaf_size': int(round(params[3])),\n",
    "        'metric': metric}\n",
    "\n",
    "knn = KNeighborsRegressor(**knn_params)\n",
    "\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, knn, Boosted=False, to_tensor=False)\n",
    "\n",
    "e_6 = 1/Time_Series_CV(model, X_C, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d26506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model = Hybrid_Pipeline(data_preprocessor, lr, knn, Boosted=True, to_tensor=False)\n",
    "model.fit(X_C, y)\n",
    "\n",
    "X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "pred_6 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f88b2",
   "metadata": {},
   "source": [
    "## Linear Regression, LSTM Regressor, Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_C = X.copy()\n",
    "\n",
    "# X_C[\"family\"] = X_C[\"family\"].cat.codes\n",
    "# X_C[\"store_nbr\"] = X_C[\"store_nbr\"].cat.codes\n",
    "# X_C[\"holiday\"] = X_C[\"holiday\"].cat.codes\n",
    "# X_C[\"event\"] = X[\"event\"].cat.codes\n",
    "# X_C[\"city\"] = X_C[\"city\"].cat.codes\n",
    "# X_C[\"state\"] = X_C[\"state\"].cat.codes\n",
    "# X_C[\"type\"] = X_C[\"type\"].cat.codes\n",
    "# X_C[\"payday\"] = X_C[\"payday\"].cat.codes\n",
    "# X_C[\"workday\"] = X_C[\"workday\"].cat.codes\n",
    "# X_C[\"holiday_description\"] = X_C[\"holiday_description\"].cat.codes\n",
    "\n",
    "# X_C = X_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "\n",
    "# X_C = X_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "# y = y.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"]).set_index([\"date\"])\n",
    "\n",
    "# X_test_C = X_test.copy()\n",
    "# X_test_C = X_test_C[[\"id\", \"store_nbr\", \"family\"] + sorted(set(X_test_C.columns)-set([\"id\", \"store_nbr\", \"family\"]))]\n",
    "# X_test_C = X_test_C.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "# X_test_C = X_test_C.set_index([\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_function(onpromotion, total_other_promo_store, total_other_city_promo, holiday,\n",
    "#                        holiday_description, event, time_since_quake, time_since_quake_sq, state, city, \n",
    "#                        dcoilwtico, day, month, workday, payday, onpromotion_lag_1, type, dayofyear, year,\n",
    "#                        onpromotion_lag_2, onpromotion_lag_3, onpromotion_lag_4, onpromotion_lag_5,\n",
    "#                        onpromotion_lag_6, onpromotion_lag_7, dcoilwtico_lag_1, dcoilwtico_lag_2, \n",
    "#                        dcoilwtico_lag_3, dcoilwtico_lag_4, dcoilwtico_lag_5, dcoilwtico_lag_6, \n",
    "#                        dcoilwtico_lag_7, sales_one_year_lag, Change_in_oil_prices, promo_last_7_days, X_C=X_C, y=y):\n",
    "    \n",
    "#     # Convert non-integer arguments to integers\n",
    "#     variable_list = [int(round(Change_in_oil_prices)), int(round(city)),int(round(day)), int(round(dayofyear)), int(round(dcoilwtico)),\n",
    "#                         int(round(dcoilwtico_lag_1)), int(round(dcoilwtico_lag_2)), int(round(dcoilwtico_lag_3)), int(round(dcoilwtico_lag_4)),\n",
    "#                         int(round(dcoilwtico_lag_5)), int(round(dcoilwtico_lag_6)), int(round(dcoilwtico_lag_7)), int(round(event)),\n",
    "#                         int(round(holiday)), int(round(holiday_description)), int(round(month)), int(round(onpromotion)),int(round(onpromotion_lag_1)),\n",
    "#                         int(round(onpromotion_lag_2)), int(round(onpromotion_lag_3)), int(round(onpromotion_lag_4)), int(round(onpromotion_lag_5)),\n",
    "#                         int(round(onpromotion_lag_6)), int(round(onpromotion_lag_7)), int(round(payday)), int(round(promo_last_7_days)),\n",
    "#                         int(round(sales_one_year_lag)), int(round(state)), int(round(time_since_quake)), int(round(time_since_quake_sq)),\n",
    "#                         int(round(total_other_city_promo)), int(round(total_other_promo_store)), int(round(type)), int(round(workday)), int(round(year))]\n",
    "    \n",
    "#     X_C = X_C.copy()\n",
    "#     column_to_remove = []\n",
    "#     for i in range(3, X_C.shape[1]):\n",
    "#         if variable_list[i-3]==0:\n",
    "#             column_to_remove.append(X_C.columns[i])\n",
    "    \n",
    "#     X_C.drop(column_to_remove, axis=1, inplace=True)\n",
    "\n",
    "#     model_2 = LSTMRegressor(Boosted=True)\n",
    "\n",
    "#     model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "#     # Use time series split for cross validation. \n",
    "#     cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "    \n",
    "#     # Create lists to append MSLE scores.\n",
    "#     valid_msle = []\n",
    "    \n",
    "#     # Dates to index through. \n",
    "#     dates = X_C.index.drop_duplicates()\n",
    "    \n",
    "    \n",
    "#     list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "#     list2 = X_C.columns\n",
    "\n",
    "#     # Convert lists to sets\n",
    "#     set1 = set(list1)\n",
    "#     set2 = set(list2)\n",
    "\n",
    "#     # Find the values in set1 that are not in set2\n",
    "#     uncommon_values = set1 - set2\n",
    "\n",
    "#     # Remove the uncommon values from list1\n",
    "#     list1 = [value for value in list1 if value not in uncommon_values]\n",
    "    \n",
    "#     numeric_transformer = [\"float\", StandardScaler()]\n",
    "#     categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "#     data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    \n",
    "    \n",
    "#     # Perform Cross-Validation to determine how model will do on unseen data.\n",
    "#     for train_index, valid_index in cv_split.split(dates):\n",
    "\n",
    "#         model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=True)\n",
    "\n",
    "#         # Index dates.\n",
    "#         date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "#         # Selecting data for y_train and y_valid.\n",
    "#         y_train = y.loc[date_train]\n",
    "#         y_valid = y.loc[date_valid]\n",
    "\n",
    "#         # Selecting data for X_train and X_valid.\n",
    "#         X_train = X_C.loc[date_train]\n",
    "#         X_valid = X_C.loc[date_valid]\n",
    "\n",
    "#         X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         X_train = X_train.set_index([\"date\"])\n",
    "#         X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "#         y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         y_train = y_train.set_index([\"date\"])\n",
    "#         y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "#         # Fitting model.\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # Create predictions for Trainning and Validation.\n",
    "#         pred = model.predict(X_valid)\n",
    "\n",
    "#         # MSE for trainning and validation. \n",
    "#         valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "\n",
    "\n",
    "#     return -float(np.sqrt(np.mean(valid_msle)))\n",
    "\n",
    "# # Define the parameter space for Bayesian optimization (each feature is a parameter)\n",
    "# params = {X_C.columns[i]: (0, 1) for i in range(3, X_C.shape[1])}\n",
    "\n",
    "# # Initialize the Bayesian optimizer\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=objective_function,\n",
    "#     pbounds=params,\n",
    "#     random_state=1,  # For reproducibility\n",
    "# )\n",
    "\n",
    "# # Perform the optimization\n",
    "# optimizer.maximize(init_points=30, n_iter=100)\n",
    "\n",
    "# variables = list(optimizer.max[\"params\"].values())\n",
    "# variables = [True, True, True] + [x>0.5 for x in variables]\n",
    "# X_C = X_C[X_C.columns[variables]]\n",
    "\n",
    "# list1 = [\"time_since_quake\", \"time_since_quake_sq\"]\n",
    "# list2 = X_C.columns\n",
    "\n",
    "# # Convert lists to sets\n",
    "# set1 = set(list1)\n",
    "# set2 = set(list2)\n",
    "\n",
    "# # Find the values in set1 that are not in set2\n",
    "# uncommon_values = set1 - set2\n",
    "\n",
    "# # Remove the uncommon values from list1\n",
    "# list1 = [value for value in list1 if value not in uncommon_values]\n",
    "\n",
    "# numeric_transformer = [\"float\", StandardScaler()]\n",
    "# categorical_transformer = [\"uint8\", OneHotEncoder(sparse=False, handle_unknown='ignore')]\n",
    "# data_preprocessor = Prepare_data(list1, [numeric_transformer, categorical_transformer])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff0d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hyperparameter_optimization(n_hidden, n_hidden_2, drop, epochs, lr):\n",
    "    \n",
    "    \n",
    "#     model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "    \n",
    "    \n",
    "#     model_2 = LSTMRegressor(n_hidden=n_hidden, n_hidden_2=n_hidden_2, drop=drop, epochs=epochs, lr=lr, Boosted=True)\n",
    "#     # Use time series split for cross validation. \n",
    "#     cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "    \n",
    "#     # Create lists to append MSLE scores.\n",
    "#     valid_msle = []\n",
    "\n",
    "#     # Dates to index through. \n",
    "#     dates = X_C.index.drop_duplicates()\n",
    "\n",
    "    \n",
    "#     # Perform Cross-Validation to determine how model will do on unseen data.\n",
    "#     for train_index, valid_index in cv_split.split(dates):\n",
    "\n",
    "#         model = Hybrid_Pipeline(data_preprocessor, model_1, model_2, Boosted=True, to_tensor=True)\n",
    "\n",
    "#         # Index dates.\n",
    "#         date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "#         # Selecting data for y_train and y_valid.\n",
    "#         y_train = y.loc[date_train]\n",
    "#         y_valid = y.loc[date_valid]\n",
    "\n",
    "#         # Selecting data for X_train and X_valid.\n",
    "#         X_train = X_C.loc[date_train]\n",
    "#         X_valid = X_C.loc[date_valid]\n",
    "\n",
    "#         X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         X_train = X_train.set_index([\"date\"])\n",
    "#         X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "#         y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#         y_train = y_train.set_index([\"date\"])\n",
    "#         y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "#         # Fitting model.\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # Create predictions for Trainning and Validation.\n",
    "#         pred = model.predict(X_valid)\n",
    "\n",
    "#         # MSE for trainning and validation. \n",
    "#         valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "\n",
    "\n",
    "#     return -float(np.sqrt(np.mean(valid_msle)))\n",
    "\n",
    "# param_bounds = {\n",
    "#     'n_hidden': (1, 20),\n",
    "#     'n_hidden_2': (0, 1),\n",
    "#     'drop': (0, 1),\n",
    "#     'epochs': (10, 500),\n",
    "#     'lr': (0, 1),\n",
    "# }\n",
    "    \n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=hyperparameter_optimization,\n",
    "#     pbounds=parambounds,\n",
    "#     random_state=1,\n",
    "# )\n",
    "\n",
    "# optimizer.maximize(init_points=30, n_iter=100,)\n",
    "# print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af424a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = optimizer.max[\"params\"]\n",
    "\n",
    "# if params[4]>1:\n",
    "#     if params[4]>2:\n",
    "#         metric = 'euclidean'\n",
    "#     else:\n",
    "#         metric = 'manhattan'\n",
    "# else:\n",
    "#     metric = 'minkowski'\n",
    "\n",
    "\n",
    "# if params[2]>1:\n",
    "#     if params[2]>2:\n",
    "#         algorithm = 'auto'\n",
    "#     else:\n",
    "#         if params[2]>3:\n",
    "#             algorithm = 'ball_tree'\n",
    "#         else:\n",
    "#             algorithm = 'kd_tree'\n",
    "# else:\n",
    "#     algorithm = 'brute'\n",
    "\n",
    "\n",
    "# if params[1]>.5:\n",
    "#     weights = 'uniform'\n",
    "# else:\n",
    "#     weights = 'distance'\n",
    "\n",
    "# param = {'n_neighbors': int(round(params[0])),\n",
    "#         'weights': weights,\n",
    "#         'algorithm': algorithm,\n",
    "#         'leaf_size': int(round(params[3])),\n",
    "#         'metric': metric}\n",
    "\n",
    "# knn = KNeighborsRegressor(**knn_params)\n",
    "\n",
    "\n",
    "# model_1 = LinearRegression(fit_intercept=False, algorithm=\"svd\", copy_X=True)\n",
    "\n",
    "# # Use time series split for cross validation. \n",
    "# cv_split = TimeSeriesSplit(n_splits = 4)\n",
    "\n",
    "# # Create lists to append MSE scores. \n",
    "# train_msle = []\n",
    "# valid_msle = []\n",
    "\n",
    "# # Dates to index through. \n",
    "# dates = X_C.index.drop_duplicates()\n",
    "# a = 0\n",
    "# # Perform Cross-Validation to determine how model will do on unseen data.\n",
    "# for train_index, valid_index in cv_split.split(dates):\n",
    "#     a = a+1\n",
    "#     print(f\"Fold {a}:\") \n",
    "#     model = Hybrid_Pipeline(data_preprocessor, lr, knn, Boosted=True, to_tensor=True)\n",
    "    \n",
    "#     # Index dates.\n",
    "#     date_train, date_valid = dates[train_index], dates[valid_index]\n",
    "\n",
    "#     # Selecting data for y_train and y_valid.\n",
    "#     y_train = y.loc[date_train]\n",
    "#     y_valid = y.loc[date_valid]\n",
    "    \n",
    "#     # Selecting data for X_train and X_valid.\n",
    "#     X_train = X_C.loc[date_train]\n",
    "#     X_valid = X_C.loc[date_valid]\n",
    "    \n",
    "#     X_train = X_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#     X_valid = X_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#     X_train = X_train.set_index([\"date\"])\n",
    "#     X_valid = X_valid.set_index([\"date\"])\n",
    "\n",
    "#     y_train = y_train.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#     y_valid = y_valid.reset_index().sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "#     y_train = y_train.set_index([\"date\"])\n",
    "#     y_valid = y_valid.set_index([\"date\"])\n",
    "\n",
    "\n",
    "#     # Fitting model.\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # Create predictions for Trainning and Validation.\n",
    "#     fit = model.predict(X_train)\n",
    "#     pred = model.predict(X_valid)\n",
    "    \n",
    "#     # MSE for trainning and validation. \n",
    "#     train_msle.append(float(mean_squared_log_error(y_train[\"sales\"], fit[\"sales\"])))\n",
    "#     valid_msle.append(float(mean_squared_log_error(y_valid[\"sales\"], pred[\"sales\"])))\n",
    "    \n",
    "#     print(f\"Training RMSLE: {cp.sqrt(mean_squared_log_error(y_train.sales, fit.sales)):.3f}, Validation RMSLE: {cp.sqrt(mean_squared_log_error(y_valid.sales, pred.sales)):.3f}\")\n",
    "\n",
    "# # Returns the square root of the average of the MSE.\n",
    "# print(\"Average Across Folds\")\n",
    "# print(f\"Training RMSLE:{np.sqrt(np.mean(train_msle)):.3f}, Validation RMSLE: {np.sqrt(np.mean(valid_msle)):.3f}\")\n",
    "\n",
    "# e_6 = 1/np.sqrt(np.mean(valid_msle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit Model\n",
    "# model = Hybrid_Pipeline(data_preprocessor, lr, xfvsgb, Boosted=True, to_tensor=False)\n",
    "# model.fit(X_C, y)\n",
    "\n",
    "# X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "# pred_7 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9dfb7",
   "metadata": {},
   "source": [
    "## Linear Regression, LSTM Regressor, Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f318129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit Model\n",
    "# model = Hybrid_Pipeline(data_preprocessor, lr, xgb, Boosted=True, to_tensor=False)\n",
    "# model.fit(X_C, y)\n",
    "\n",
    "# X_test_C = X_test_C[X_test_C.columns[variables]]\n",
    "# pred_8 = model.predict(X_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd116d2-42fc-489e-8167-20406eb2c7d1",
   "metadata": {},
   "source": [
    "## Final Predictions and Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b6096-78e0-4052-b58a-398d9c2b54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "e_sum = e_1+e_2+e_3+e_4+e_5+e_6\n",
    "#+e_7+e_8\n",
    "\n",
    "ensembled_pred = pred_1*e_1/e_sum + pred_2*e_2/e_sum + pred_3*e_3/e_sum + pred_4*e_4/e_sum + pred_5*e_5/e_sum + pred_6*e_6/e_sum\n",
    "#+ pred_7*e_7/e_sum + pred_8*e_8/e_sum\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca14a1b-1fff-4e6e-adec-26c42425112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_pred.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61c910-4a78-462f-ad44-f0c8d196abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.competition_submit('submission.csv','1st API Submission','store-sales-time-series-forecasting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e8537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (rapidsai)",
   "language": "python",
   "name": "rapidsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
